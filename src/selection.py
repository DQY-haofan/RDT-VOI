"""
ä¼ æ„Ÿå™¨é€‰æ‹©ç®—æ³•é›†åˆ - å®Œæ•´ç‰ˆæœ¬

åŒ…æ‹¬ï¼š
1. greedy_mi - è´ªå¿ƒäº’ä¿¡æ¯
2. greedy_aopt - è´ªå¿ƒA-optimalï¼ˆè¿¹æœ€å°åŒ–ï¼‰
3. greedy_evi_myopic_fast - å¿«é€Ÿå†³ç­–æ„ŸçŸ¥EVIï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
4. maxmin_k_center - æœ€å¤§æœ€å°è¦†ç›–
5. uniform_selection - å‡åŒ€éšæœºé€‰æ‹©ï¼ˆå‘åå…¼å®¹ï¼‰
6. random_selection - é€†æˆæœ¬åŠ æƒéšæœºé€‰æ‹©ï¼ˆå‘åå…¼å®¹ï¼‰

ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ - 2025-01-25
"""

import numpy as np
import scipy.sparse as sp
from typing import List, Tuple
from dataclasses import dataclass
from sensors import Sensor, assemble_H_R


@dataclass
class SelectionResult:
    """ä¼ æ„Ÿå™¨é€‰æ‹©ç»“æœ"""
    selected_ids: List[int]
    objective_values: List[float]
    marginal_gains: List[float]
    total_cost: float
    method_name: str


# =====================================================================
# 1. Greedy MIï¼ˆäº’ä¿¡æ¯ï¼‰
# =====================================================================
def greedy_mi(sensors, k: int, Q_pr, costs: np.ndarray = None,
              lazy: bool = True, batch_size: int = 1) -> 'SelectionResult':
    """
    Greedy mutual information maximization (æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬)

    Args:
        sensors: å€™é€‰ä¼ æ„Ÿå™¨åˆ—è¡¨
        k: è¦é€‰æ‹©çš„ä¼ æ„Ÿå™¨æ•°é‡
        Q_pr: å…ˆéªŒç²¾åº¦çŸ©é˜µ
        costs: ä¼ æ„Ÿå™¨æˆæœ¬æ•°ç»„ï¼ˆé•¿åº¦å¿…é¡»ç­‰äºlen(sensors)ï¼‰
        lazy: æ˜¯å¦ä½¿ç”¨lazy evaluation
        batch_size: æ‰¹é‡å¤§å°
    """
    import numpy as np
    import scipy.sparse as sp
    from inference import SparseFactor

    n = Q_pr.shape[0]
    C = len(sensors)

    # âœ… ç¡®ä¿costsç»´åº¦æ­£ç¡®
    if costs is None:
        costs = np.ones(C, dtype=float)
    else:
        costs = np.asarray(costs, dtype=float)
        if len(costs) != C:
            raise ValueError(f"Cost array length {len(costs)} doesn't match sensor count {C}")

    # åˆå§‹åŒ–
    selected = []
    marginal_gains = []
    objective_values = []
    total_cost = 0.0

    # é¢„è®¡ç®—æ‰€æœ‰å€™é€‰çš„hå‘é‡ï¼ˆç¨ å¯†ï¼‰
    H_rows = []
    R_list = []
    for s in sensors:
        # æ„é€ hå‘é‡ï¼ˆnç»´ï¼‰
        h = np.zeros(n)
        h[s.idxs] = s.weights
        H_rows.append(h)
        R_list.append(s.noise_var)

    H_rows = np.array(H_rows)  # (C, n)
    R_list = np.array(R_list)  # (C,)

    # åˆå§‹å› å­
    factor = SparseFactor(Q_pr)

    # æ‰¹é‡è®¡ç®—åˆå§‹MIï¼ˆå¦‚æœéœ€è¦é¢„ç­›é€‰ï¼‰
    if lazy and C > 100:
        # ä¸€æ¬¡æ€§æ±‚è§£ Z = Î£ H^T
        Z = factor.solve_multi(H_rows.T)  # (n, C)

        # âœ… å…³é”®ä¿®å¤ï¼šè®¡ç®— h^T Î£ h = sum over n (H[c,n] * Z[n,c])
        # H_rows: (C, n)
        # Z.T: (C, n)
        # quad[c] = sum_n H_rows[c,n] * Z.T[c,n] = sum_n H_rows[c,n] * Z[n,c]
        quad = np.sum(H_rows * Z.T, axis=1)  # (C,) âœ… æ­£ç¡®ï¼

        mi_values = 0.5 * np.log1p(quad / R_list)
    else:
        mi_values = None

    # Greedyå¾ªç¯
    alive = np.ones(C, dtype=bool)

    for step in range(k):
        best_idx = -1
        best_gain = -np.inf
        best_mi = 0.0

        # å€™é€‰è¯„ä¼°
        candidates = np.where(alive)[0]

        for idx in candidates:
            h = H_rows[idx]  # (n,)
            r = R_list[idx]

            # è®¡ç®—è¾¹é™…MI
            z = factor.solve(h)  # (n,)
            quad = np.dot(h, z)
            mi = 0.5 * np.log1p(quad / r)

            # æˆæœ¬å½’ä¸€åŒ–å¾—åˆ†
            gain = mi / costs[idx]

            if gain > best_gain:
                best_gain = gain
                best_idx = idx
                best_mi = mi

        if best_idx < 0 or best_gain <= 0:
            break

        # è®°å½•é€‰æ‹©
        selected.append(int(best_idx))
        marginal_gains.append(float(best_mi))
        total_cost += float(costs[best_idx])
        objective_values.append(
            objective_values[-1] + best_mi if objective_values else best_mi
        )

        # æ›´æ–°ï¼šrank-1å¢é‡
        h_star = H_rows[best_idx]
        r_star = R_list[best_idx]

        # Rank-1 update: Q_new = Q + (1/r) * h h^T
        factor.rank1_update(h_star, weight=1.0 / r_star)

        # æ ‡è®°å·²é€‰
        alive[best_idx] = False

    return SelectionResult(
        selected_ids=selected,
        objective_values=objective_values,
        marginal_gains=marginal_gains,
        total_cost=total_cost,
        method_name="Greedy-MI"
    )
# =====================================================================
# 2. Greedy A-optimalï¼ˆè¿¹æœ€å°åŒ–ï¼‰
# =====================================================================

def greedy_aopt(sensors, k: int, Q_pr, costs: np.ndarray = None,
                n_probes: int = 16, use_cost: bool = True) -> 'SelectionResult':
    """
    Greedy A-optimal design (trace minimization)

    ä½¿ç”¨ Hutchinson++ ä¼°è®¡ trace(Î£)
    """
    import numpy as np
    from inference import SparseFactor

    n = Q_pr.shape[0]
    C = len(sensors)

    # âœ… ä¿®å¤costsç»´åº¦
    if costs is None:
        costs = np.ones(C, dtype=float)
    else:
        costs = np.asarray(costs, dtype=float)
        if len(costs) != C:
            raise ValueError(f"Cost array length {len(costs)} doesn't match sensor count {C}")

    # åˆå§‹åŒ–
    selected = []
    marginal_gains = []
    objective_values = []
    total_cost = 0.0

    # é¢„è®¡ç®—hå‘é‡
    H_rows = []
    R_list = []
    for s in sensors:
        h = np.zeros(n)
        h[s.idxs] = s.weights
        H_rows.append(h)
        R_list.append(s.noise_var)

    H_rows = np.array(H_rows)  # (C, n)
    R_list = np.array(R_list)

    # åˆå§‹å› å­å’Œtraceä¼°è®¡
    factor = SparseFactor(Q_pr)

    # Hutchinson++ probes
    rng = np.random.default_rng(42)
    probes = rng.standard_normal((n, n_probes))
    Z_probes = factor.solve_multi(probes)  # (n, n_probes)
    trace_current = np.mean(np.sum(probes * Z_probes, axis=0))

    alive = np.ones(C, dtype=bool)

    for step in range(k):
        best_idx = -1
        best_gain = -np.inf
        best_reduction = 0.0

        candidates = np.where(alive)[0]

        for idx in candidates:
            h = H_rows[idx]
            r = R_list[idx]

            # è®¡ç®—trace reduction
            z = factor.solve(h)
            quad = np.dot(h, z)

            # Sherman-Morrison: trace(Î£') = trace(Î£) - quad/(r + quad)
            denom = r + quad
            if denom > 1e-12:
                reduction = quad / denom
                gain = reduction / costs[idx] if use_cost else reduction

                if gain > best_gain:
                    best_gain = gain
                    best_idx = idx
                    best_reduction = reduction

        if best_idx < 0 or best_gain <= 0:
            break

        # è®°å½•
        selected.append(int(best_idx))
        marginal_gains.append(float(best_reduction))
        total_cost += float(costs[best_idx])
        trace_current -= best_reduction
        objective_values.append(float(trace_current))

        # Rank-1 update
        h_star = H_rows[best_idx]
        r_star = R_list[best_idx]
        factor.rank1_update(h_star, weight=1.0 / r_star)

        alive[best_idx] = False

    return SelectionResult(
        selected_ids=selected,
        objective_values=objective_values,
        marginal_gains=marginal_gains,
        total_cost=total_cost,
        method_name="Greedy-Aopt"
    )


# =====================================================================
# 3. ğŸ”¥ Greedy EVI Myopic Fastï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
# =====================================================================

def greedy_evi_myopic_fast(
        sensors,
        k: int,
        Q_pr,
        mu_pr: np.ndarray,
        decision_config,
        test_idx: np.ndarray,
        costs: np.ndarray = None,
        n_y_samples: int = 0,
        use_cost: bool = True,
        mi_prescreen: bool = True,
        keep_fraction: float = 0.25,
        rng: np.random.Generator = None,
        verbose: bool = False
) -> 'SelectionResult':
    """
    ğŸ”¥ å¿«é€Ÿç‰ˆ Myopic EVI (å†³ç­–æ„ŸçŸ¥) - ä½¿ç”¨ MI é¢„ç­› + rank-1 æ›´æ–°

    å…³é”®ä¼˜åŒ–ï¼š
    1. ä¸ä¸ºæ¯ä¸ªå€™é€‰åšå› å­åŒ–ï¼›æ¯æ­¥ä»…ä¸€æ¬¡ç¨€ç–è§£ + çº¯å‘é‡ä»£æ•°
    2. åéªŒåæ–¹å·®ä¸ y æ— å…³ï¼Œç”¨ rank-1 é—­å¼æ›´æ–°
    3. åªåœ¨ test_idx ä¸Šè®¡ç®—é£é™©
    4. MIé¢„ç­›é€‰å‡å°‘å€™é€‰æ•°é‡
    """
    import numpy as np
    import scipy.sparse as sp
    from inference import SparseFactor
    from decision import expected_loss

    if rng is None:
        rng = np.random.default_rng()

    n = Q_pr.shape[0]
    C = len(sensors)

    if costs is None:
        costs = np.array([s.cost for s in sensors], dtype=float)

    if verbose:
        print(f"\n  ğŸš€ Fast Greedy-EVI: n={n}, candidates={C}, budget={k}, test={len(test_idx)}")

    # ---------------------------
    # 1. å…ˆéªŒå› å­ä¸æµ‹è¯•é›†å…ˆéªŒæ–¹å·®
    # ---------------------------
    F = SparseFactor(Q_pr)

    # ç”¨å•ä½åŸºçš„å­é›†ä¸€æ¬¡å¤šRHSè§£å‡º diag(Î£)_test
    I_sub = np.zeros((n, len(test_idx)), dtype=float)
    I_sub[test_idx, np.arange(len(test_idx))] = 1.0
    Z_sub = F.solve_multi(I_sub)
    diag_test = np.einsum('ij,ij->j', Z_sub[test_idx, :], I_sub[test_idx, :])
    diag_test = np.maximum(diag_test, 1e-12)
    sigma_test = np.sqrt(diag_test)
    mu_test = mu_pr[test_idx].copy()

    # è®¡ç®—å…ˆéªŒé£é™©
    prior_risk = expected_loss(
        mu_test,
        sigma_test,
        decision_config,
        test_indices=np.arange(len(test_idx))
    )

    if verbose:
        print(f"    Prior risk: Â£{prior_risk:.2f}")
        print(f"    Prior Ïƒ on test: mean={sigma_test.mean():.4f}, std={sigma_test.std():.4f}")

    # ---------------------------
    # 2. é¢„å–æ¯ä¸ªå€™é€‰çš„ç¨€ç– hï¼ˆè¡Œå‘é‡ï¼‰
    # ---------------------------
    idx_list = [s.idxs for s in sensors]
    w_list = [s.weights for s in sensors]
    r_list = np.array([s.noise_var for s in sensors], dtype=float)

    # æŠŠæ‰€æœ‰å€™é€‰åˆæˆç¨ å¯† Hï¼ˆè¡Œ=å€™é€‰ï¼Œåˆ—=nï¼‰
    H_dense = np.stack(
        [np.bincount(idxs, weights=w, minlength=n) for idxs, w in zip(idx_list, w_list)],
        axis=0
    )  # C Ã— n

    # ---------------------------
    # 3. MI é¢„ç­›ï¼šä¸€æ¬¡å¤š RHS å¾—åˆ° Z=Î£ H^T
    # ---------------------------
    keep_mask = np.ones(C, dtype=bool)
    original_indices = np.arange(C)  # ä¿å­˜åŸå§‹ç´¢å¼•æ˜ å°„

    if mi_prescreen and C > 50:
        if verbose:
            print(f"    ğŸ” MI prescreening over {C} candidates ...")

        Z = F.solve_multi(H_dense.T)  # n Ã— C

        # æ¯ä¸ªå€™é€‰çš„ quad = h^T Î£ h = h^T z_h
        quad = np.einsum('ij,ij->j', H_dense, Z.T)  # (C,)
        mi = 0.5 * np.log1p(quad / r_list)

        n_keep = max(20, int(C * keep_fraction))
        keep_idx = np.argpartition(mi, -n_keep)[-n_keep:]
        keep_mask[:] = False
        keep_mask[keep_idx] = True

        # æ›´æ–°æ‰€æœ‰æ•°æ®ç»“æ„
        H_dense = H_dense[keep_mask, :]
        r_list = r_list[keep_mask]
        costs = costs[keep_mask]
        idx_list = [idx_list[i] for i in range(C) if keep_mask[i]]
        w_list = [w_list[i] for i in range(C) if keep_mask[i]]
        original_indices = original_indices[keep_mask]
        Z = Z[:, keep_mask]  # n Ã— C_new
        C = H_dense.shape[0]

        if verbose:
            print(f"    âœ“ kept {C} ({100 * C / len(keep_mask):.0f}%), "
                  f"MIâˆˆ[{mi[keep_mask].min():.3f},{mi[keep_mask].max():.3f}] nats")
    else:
        Z = F.solve_multi(H_dense.T)  # n Ã— C

    # ---------------------------
    # 4. Greedy å¾ªç¯ï¼šæ¯æ­¥ 1 æ¬¡è§£ + å‘é‡ä»£æ•°
    # ---------------------------
    selected = []
    mg = []
    obj = []
    tot_cost = 0.0
    alive = np.ones(C, dtype=bool)  # å½“å‰æœªè¢«é€‰ä¸­çš„å€™é€‰

    for step in range(k):
        if verbose:
            print(f"    Step {step + 1}/{k}")

        # å¯¹æ‰€æœ‰ä»åœ¨æ± å†…çš„å€™é€‰ï¼Œè®¡ç®—"åŠ å®ƒåçš„ posterior é£é™©"
        # posterior diag on test: diag' = diag - (z_h_test^2)/(r + h^T z_h)
        zt = Z[test_idx, :]  # m_test Ã— C
        num = np.sum(zt * zt, axis=0)  # (C,)
        denom = r_list + np.einsum('ij,ij->j', H_dense, Z.T)  # (C,)
        denom = np.maximum(denom, 1e-12)

        # é€å€™é€‰å¾—åˆ° Î£' çš„ test å¯¹è§’
        diag_post_all = diag_test[:, None] - (zt * zt) / denom[None, :]  # m_test Ã— C
        diag_post_all = np.maximum(diag_post_all, 1e-12)

        # è®¡ç®— posterior é£é™©
        sigma_post_all = np.sqrt(diag_post_all)

        # å‘é‡åŒ–è®¡ç®—é£é™©
        post_risk = np.empty(C)
        for j in range(C):
            if not alive[j]:
                post_risk[j] = np.inf
                continue
            post_risk[j] = expected_loss(
                mu_test,
                sigma_post_all[:, j],
                decision_config,
                test_indices=np.arange(len(test_idx))
            )

        # è®¡ç®— EVI å¢ç›Š
        evi_gain = prior_risk - post_risk  # (C,)
        score = evi_gain / costs if use_cost else evi_gain
        best = int(np.argmax(score))

        if not np.isfinite(score[best]) or score[best] <= 0:
            if verbose:
                print("      âš ï¸  no positive EVI gain; stopping.")
            break

        # è®°å½•ï¼ˆä½¿ç”¨åŸå§‹ç´¢å¼•ï¼‰
        selected.append(int(original_indices[best]))
        mg.append(float(evi_gain[best]))
        tot_cost += float(costs[best])
        obj.append(obj[-1] + mg[-1] if obj else mg[-1])

        if verbose:
            print(f"      pick #{step + 1}: cand={original_indices[best]}, "
                  f"Î”EVI=Â£{mg[-1]:.2f}, cost=Â£{costs[best]:.0f}")

        # ---- Rank-1 æ›´æ–° Z ä¸ diag_test ----
        z_star = Z[:, best]
        h_star = H_dense[best, :]
        den = denom[best]

        # æ›´æ–° test å¯¹è§’
        diag_test = diag_test - (z_star[test_idx] ** 2) / den
        diag_test = np.maximum(diag_test, 1e-12)
        sigma_test = np.sqrt(diag_test)
        prior_risk = expected_loss(
            mu_test,
            sigma_test,
            decision_config,
            test_indices=np.arange(len(test_idx))
        )

        # æ›´æ–° Zï¼šZ' = Z - z_* (h_*^T Z)/den
        c = h_star @ Z  # (C,) = (n,) @ (nÃ—C)
        Z -= np.outer(z_star, c) / den  # (nÃ—1) (1Ã—C) / scalar

        # æ ‡è®°è¯¥å€™é€‰å¤±æ•ˆ
        alive[best] = False

    return SelectionResult(
        selected_ids=selected,
        objective_values=obj,
        marginal_gains=mg,
        total_cost=tot_cost,
        method_name="Greedy-EVI-fast"
    )
# =====================================================================
# 4. Maxmin k-center
# =====================================================================

def maxmin_k_center(sensors, k: int, coords: np.ndarray,
                    costs: np.ndarray = None, use_cost: bool = True) -> 'SelectionResult':
    """
    Maxmin k-center (spatial coverage)

    é€‰æ‹©ä½¿æœ€å°è¦†ç›–è·ç¦»æœ€å¤§åŒ–çš„ä¼ æ„Ÿå™¨
    """
    import numpy as np
    from scipy.spatial.distance import cdist

    C = len(sensors)

    # âœ… ä¿®å¤costsç»´åº¦
    if costs is None:
        costs = np.ones(C, dtype=float)
    else:
        costs = np.asarray(costs, dtype=float)
        if len(costs) != C:
            raise ValueError(f"Cost array length {len(costs)} doesn't match sensor count {C}")

    # è·å–ä¼ æ„Ÿå™¨ä½ç½®ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªidxsä½œä¸ºä»£è¡¨ï¼‰
    sensor_coords = np.array([coords[s.idxs[0]] for s in sensors])

    # è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¼ æ„Ÿå™¨çš„è·ç¦»çŸ©é˜µ
    dist_matrix = cdist(coords, sensor_coords)  # (n, C)

    selected = []
    total_cost = 0.0

    # åˆå§‹åŒ–ï¼šé€‰æ‹©ç¦»æ‰€æœ‰ç‚¹å¹³å‡è·ç¦»æœ€è¿œçš„ä¼ æ„Ÿå™¨
    avg_dist = dist_matrix.mean(axis=0)
    if use_cost:
        score = avg_dist / costs
    else:
        score = avg_dist
    first = int(np.argmax(score))
    selected.append(first)
    total_cost += float(costs[first])

    # è·Ÿè¸ªæ¯ä¸ªç‚¹åˆ°å·²é€‰ä¼ æ„Ÿå™¨çš„æœ€å°è·ç¦»
    min_dist = dist_matrix[:, first].copy()

    for step in range(1, k):
        # å¯¹æ¯ä¸ªæœªé€‰ä¼ æ„Ÿå™¨ï¼Œè®¡ç®—å¦‚æœé€‰å®ƒï¼Œæœ€å°è·ç¦»ä¼šå¦‚ä½•å˜åŒ–
        best_idx = -1
        best_score = -np.inf

        for idx in range(C):
            if idx in selected:
                continue

            # è®¡ç®—æ–°çš„æœ€å°è·ç¦»
            new_min_dist = np.minimum(min_dist, dist_matrix[:, idx])

            # è¯„åˆ†ï¼šæœ€å°è·ç¦»çš„æœ€å°å€¼ï¼ˆmaxminå‡†åˆ™ï¼‰
            maxmin_dist = new_min_dist.min()

            if use_cost:
                score = maxmin_dist / costs[idx]
            else:
                score = maxmin_dist

            if score > best_score:
                best_score = score
                best_idx = idx

        if best_idx < 0:
            break

        selected.append(int(best_idx))
        total_cost += float(costs[best_idx])
        min_dist = np.minimum(min_dist, dist_matrix[:, best_idx])

    return SelectionResult(
        selected_ids=selected,
        objective_values=[0.0] * len(selected),  # ä¸è®°å½•ç›®æ ‡å€¼
        marginal_gains=[0.0] * len(selected),
        total_cost=total_cost,
        method_name="Maxmin"
    )


# =====================================================================
# 5. Uniform Selectionï¼ˆå‘åå…¼å®¹ï¼‰
# =====================================================================

def uniform_selection(sensors: List[Sensor], k: int, Q_pr: sp.spmatrix = None,
                     mu_pr: np.ndarray = None, rng: np.random.Generator = None) -> SelectionResult:
    """
    å‡åŒ€éšæœºé€‰æ‹©ï¼ˆä¸è€ƒè™‘ä¿¡æ¯é‡ï¼‰

    å‘åå…¼å®¹å‡½æ•°ï¼šä¾› main.py ç›´æ¥å¯¼å…¥ä½¿ç”¨

    Args:
        sensors: å€™é€‰ä¼ æ„Ÿå™¨åˆ—è¡¨
        k: é¢„ç®—
        Q_pr: å…ˆéªŒç²¾åº¦çŸ©é˜µï¼ˆæœªä½¿ç”¨ï¼‰
        mu_pr: å…ˆéªŒå‡å€¼ï¼ˆæœªä½¿ç”¨ï¼‰
        rng: éšæœºæ•°ç”Ÿæˆå™¨

    Returns:
        SelectionResult
    """
    if rng is None:
        rng = np.random.default_rng()

    n_sensors = len(sensors)

    if k > n_sensors:
        k = n_sensors

    selected_ids = rng.choice(n_sensors, size=k, replace=False).tolist()
    total_cost = sum(sensors[i].cost for i in selected_ids)

    return SelectionResult(
        selected_ids=selected_ids,
        objective_values=[0.0] * k,
        marginal_gains=[0.0] * k,
        total_cost=total_cost,
        method_name="Uniform"
    )


# =====================================================================
# 6. Random Selectionï¼ˆå‘åå…¼å®¹ï¼‰
# =====================================================================

def random_selection(sensors: List[Sensor], k: int, Q_pr: sp.spmatrix = None,
                    mu_pr: np.ndarray = None, rng: np.random.Generator = None) -> SelectionResult:
    """
    éšæœºé€‰æ‹©ï¼ˆé€†æˆæœ¬åŠ æƒï¼‰

    å‘åå…¼å®¹å‡½æ•°ï¼šä¾› main.py ç›´æ¥å¯¼å…¥ä½¿ç”¨

    Args:
        sensors: å€™é€‰ä¼ æ„Ÿå™¨åˆ—è¡¨
        k: é¢„ç®—
        Q_pr: å…ˆéªŒç²¾åº¦çŸ©é˜µï¼ˆæœªä½¿ç”¨ï¼‰
        mu_pr: å…ˆéªŒå‡å€¼ï¼ˆæœªä½¿ç”¨ï¼‰
        rng: éšæœºæ•°ç”Ÿæˆå™¨

    Returns:
        SelectionResult
    """
    if rng is None:
        rng = np.random.default_rng()

    n_sensors = len(sensors)
    costs = np.array([s.cost for s in sensors], dtype=float)

    # é€†æˆæœ¬åŠ æƒï¼ˆä¾¿å®œçš„ä¼ æ„Ÿå™¨æ›´å¯èƒ½è¢«é€‰ä¸­ï¼‰
    weights = 1.0 / (costs + 1.0)
    weights = weights / weights.sum()

    if k > n_sensors:
        k = n_sensors

    selected_ids = rng.choice(n_sensors, size=k, replace=False, p=weights).tolist()
    total_cost = sum(sensors[i].cost for i in selected_ids)

    return SelectionResult(
        selected_ids=selected_ids,
        objective_values=[0.0] * k,
        marginal_gains=[0.0] * k,
        total_cost=total_cost,
        method_name="Random"
    )