"""
Decision-theoretic value mapping and Expected Value of Information.
ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ - ç»Ÿä¸€ Bayes æœ€ä¼˜é˜ˆå€¼å…¬å¼ï¼Œé¿å…ä¸åŒå‡½æ•°é‡Œç‰ˆæœ¬ä¸ä¸€
"""

import numpy as np
from scipy.stats import norm
from typing import Tuple, List
import warnings


def get_unified_prob_threshold(L_FP: float, L_FN: float, L_TP: float, L_TN: float = 0.0) -> float:
    """
    ğŸ”¥ ç»Ÿä¸€çš„ Bayes æœ€ä¼˜æ¦‚ç‡é˜ˆå€¼è®¡ç®—

    ä½¿ç”¨é€šç”¨å…¬å¼ï¼šp_T = (L_FP - L_TN) / [(L_FP - L_TN) + (L_FN - L_TP)]

    Args:
        L_FP: False positive cost
        L_FN: False negative cost
        L_TP: True positive cost
        L_TN: True negative cost (default 0)

    Returns:
        p_T: Optimal probability threshold
    """
    numerator = L_FP - L_TN
    denominator = (L_FP - L_TN) + (L_FN - L_TP)

    if abs(denominator) < 1e-10:
        warnings.warn("Near-singular decision cost matrix, using p_T=0.5")
        return 0.5

    p_T = numerator / denominator

    # å¥åº·æ£€æŸ¥
    if not (0 <= p_T <= 1):
        warnings.warn(f"Invalid p_T={p_T:.3f}, clamping to [0,1]")
        p_T = np.clip(p_T, 0.0, 1.0)

    return p_T


def conditional_risk(mu: float, sigma: float,
                          tau: float, L_FP: float, L_FN: float, L_TP: float,
                          L_TN: float = 0.0) -> float:
    """
    ğŸ”¥ ç´§æ€¥ä¿®å¤ç‰ˆï¼šç¡®ä¿æ°¸è¿œä¸è¿”å›None

    Bayes-optimal conditional risk.
    """
    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥ï¼šè¾“å…¥å‚æ•°
    if any(x is None for x in [mu, sigma, tau, L_FP, L_FN, L_TP]):
        raise ValueError(
            f"conditional_risk: None parameter detected! "
            f"mu={mu}, sigma={sigma}, tau={tau}, "
            f"L_FP={L_FP}, L_FN={L_FN}, L_TP={L_TP}"
        )

    if sigma <= 0:
        # Degenerate case: certain knowledge
        if mu > tau:
            return float(L_TP)  # Maintain (correct)
        else:
            return float(L_TN if L_TN is not None else 0.0)

    # Posterior failure probability
    try:
        p_f = 1.0 - norm.cdf((tau - mu) / sigma)
    except Exception as e:
        warnings.warn(f"norm.cdf failed: {e}, using p_f=0.5")
        p_f = 0.5

    # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„ Bayes æœ€ä¼˜æ¦‚ç‡é˜ˆå€¼
    p_T = get_unified_prob_threshold(L_FP, L_FN, L_TP, L_TN)

    # Conditional risk for each action
    risk_no_action = p_f * L_FN + (1 - p_f) * (L_TN if L_TN is not None else 0.0)
    risk_action = p_f * L_TP + (1 - p_f) * L_FP

    # Bayes-optimal risk
    result = float(min(risk_no_action, risk_action))

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿è¿”å›å€¼æœ‰æ•ˆ
    if result is None or not np.isfinite(result):
        warnings.warn(f"conditional_risk: invalid result {result}, returning 0.0")
        return 0.0

    return result


def expected_loss(mu_post: np.ndarray,
                       sigma_post: np.ndarray,
                       decision_config,
                       test_indices: np.ndarray = None,
                       tau: float = None) -> float:
    """
    ğŸ”¥ ç´§æ€¥ä¿®å¤ç‰ˆï¼šç¡®ä¿æ°¸è¿œä¸è¿”å›None

    Compute expected economic loss averaged over test set.
    """
    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥1ï¼šç¡®ä¿è¾“å…¥ä¸æ˜¯None
    if mu_post is None or sigma_post is None or decision_config is None:
        raise ValueError("expected_loss: None input detected!")

    if test_indices is None:
        test_indices = np.arange(len(mu_post))

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥2ï¼šç¡®ä¿test_indicesæœ‰æ•ˆ
    if len(test_indices) == 0:
        warnings.warn("expected_loss: empty test_indices, returning 0.0")
        return 0.0

    # ğŸ”¥ æ”¹è¿›çš„é˜ˆå€¼è·å–é€»è¾‘
    if tau is None:
        if hasattr(decision_config, 'tau_iri') and decision_config.tau_iri is not None:
            tau = decision_config.tau_iri
        elif hasattr(decision_config, 'get_threshold'):
            tau = decision_config.get_threshold(mu_post)
            # ç¼“å­˜ä»¥é¿å…é‡å¤è®¡ç®—
            if not hasattr(decision_config, '_tau_warning_shown'):
                warnings.warn(
                    f"Computing dynamic threshold on-the-fly (tau={tau:.3f}). "
                    f"Pre-compute tau_iri in main() for better performance.",
                    category=UserWarning, stacklevel=2
                )
                decision_config._tau_warning_shown = True
        else:
            raise ValueError(
                "Decision threshold not configured. "
                "Set either tau_iri or tau_quantile in config.yaml"
            )

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥3ï¼šç¡®ä¿tauæ˜¯æœ‰æ•ˆæ•°å€¼
    if tau is None or not np.isfinite(tau):
        raise ValueError(f"Invalid tau: {tau}")

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥4ï¼šç¡®ä¿æŸå¤±å‚æ•°ä¸æ˜¯None
    L_FP = decision_config.L_FP_gbp
    L_FN = decision_config.L_FN_gbp
    L_TP = decision_config.L_TP_gbp
    L_TN = getattr(decision_config, 'L_TN_gbp', 0.0)

    if any(x is None for x in [L_FP, L_FN, L_TP]):
        raise ValueError(
            f"Loss parameters contain None: "
            f"L_FP={L_FP}, L_FN={L_FN}, L_TP={L_TP}, L_TN={L_TN}"
        )

    # è®¡ç®—é£é™©
    risks = []
    for i in test_indices:
        risk = conditional_risk(
            mu_post[i], sigma_post[i],
            tau, L_FP, L_FN, L_TP, L_TN
        )

        # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥5ï¼šç¡®ä¿å•ä¸ªé£é™©ä¸æ˜¯None
        if risk is None:
            warnings.warn(f"conditional_risk returned None at index {i}, using 0.0")
            risk = 0.0

        risks.append(risk)

    risks_array = np.array(risks)

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥6ï¼šç¡®ä¿ç»“æœæœ‰æ•ˆ
    if len(risks_array) == 0:
        warnings.warn("expected_loss: no valid risks computed, returning 0.0")
        return 0.0

    result = float(risks_array.mean())

    # ğŸ”¥ é˜²å¾¡æ€§æ£€æŸ¥7ï¼šç¡®ä¿è¿”å›å€¼ä¸æ˜¯Noneæˆ–NaN
    if result is None or not np.isfinite(result):
        warnings.warn(f"expected_loss: invalid result {result}, returning 0.0")
        return 0.0

    return result


def expected_loss_batch(mu_post_batch: np.ndarray,
                             sigma_post_batch: np.ndarray,
                             decision_config,
                             test_indices: np.ndarray = None) -> np.ndarray:
    """
    ğŸ”¥ ä¿®å¤ç‰ˆï¼šæ‰¹é‡è®¡ç®— expected lossï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ï¼‰- åŠ é€Ÿ 20-50x

    å…³é”®æ”¹è¿›ï¼š
    - ä½¿ç”¨ç»Ÿä¸€çš„é˜ˆå€¼è®¡ç®—
    - ä¼˜åŒ–çš„å‘é‡åŒ–å®ç°
    - å‡å°‘é‡å¤è­¦å‘Š
    """
    # ğŸ”¥ æ”¹è¿›çš„é˜ˆå€¼è·å–é€»è¾‘
    if hasattr(decision_config, 'tau_iri') and decision_config.tau_iri is not None:
        tau = decision_config.tau_iri
    elif hasattr(decision_config, 'tau_quantile') and decision_config.tau_quantile is not None:
        if mu_post_batch.ndim == 1:
            tau = float(np.quantile(mu_post_batch, decision_config.tau_quantile))
        else:
            tau = float(np.quantile(mu_post_batch, decision_config.tau_quantile))

        # åªåœ¨é¦–æ¬¡è®¡ç®—æ—¶è­¦å‘Š
        if not hasattr(decision_config, '_batch_tau_warning_shown'):
            warnings.warn(
                f"tau_iri not set, using dynamic threshold: tau = {tau:.3f}. "
                "For better performance, set tau_iri in main() before evaluation."
            )
            decision_config._batch_tau_warning_shown = True
    else:
        raise ValueError(
            "Decision threshold not configured. "
            "Set either tau_iri or tau_quantile in config.yaml"
        )

    L_FP = decision_config.L_FP_gbp
    L_FN = decision_config.L_FN_gbp
    L_TP = decision_config.L_TP_gbp
    L_TN = getattr(decision_config, 'L_TN_gbp', 0.0)

    # é˜²æ­¢é™¤é›¶
    sigma_safe = np.maximum(sigma_post_batch, 1e-12)

    # å‘é‡åŒ–è®¡ç®—åéªŒå¤±æ•ˆæ¦‚ç‡
    z_scores = (tau - mu_post_batch) / sigma_safe
    p_fail = 1.0 - norm.cdf(z_scores)

    # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„ Bayes-optimal å†³ç­–é˜ˆå€¼
    p_T = get_unified_prob_threshold(L_FP, L_FN, L_TP, L_TN)

    # ä¸¤ç§è¡ŒåŠ¨çš„æ¡ä»¶é£é™©
    risk_no_action = p_fail * L_FN + (1 - p_fail) * L_TN
    risk_action = p_fail * L_TP + (1 - p_fail) * L_FP

    # Bayes-optimal é£é™©ï¼ˆé€ç‚¹å–æœ€å°ï¼‰
    optimal_risk = np.minimum(risk_no_action, risk_action)

    # å¦‚æœæ˜¯ 2D (n_test, n_candidates)ï¼Œæ²¿æµ‹è¯•ç‚¹è½´æ±‚å¹³å‡
    if optimal_risk.ndim == 2:
        return optimal_risk.mean(axis=0)  # (n_candidates,)
    else:
        return optimal_risk.mean()  # æ ‡é‡


def evi_monte_carlo(Q_pr, mu_pr, H, R_diag, decision_config,
                         n_samples: int = 500,
                         rng: np.random.Generator = None) -> float:
    """
    ğŸ”¥ ä¿®å¤ç‰ˆï¼šä¸¥è°¨çš„ EVI Monte Carlo è¿‘ä¼¼ï¼ˆä½¿ç”¨ç»Ÿä¸€é˜ˆå€¼ï¼‰

    å…³é”®ä¿®å¤ï¼š
    1. ä½¿ç”¨ç»Ÿä¸€çš„æ¦‚ç‡é˜ˆå€¼è®¡ç®—
    2. é¢„ç¼“å­˜å†³ç­–é˜ˆå€¼ï¼Œé¿å…é‡å¤è®¡ç®—
    3. å®Œæ•´çš„ priorâ†’observationâ†’posteriorâ†’é£é™©å·® æµç¨‹
    """
    from inference import SparseFactor, compute_posterior, compute_posterior_variance_diagonal
    from spatial_field import sample_gmrf

    if rng is None:
        rng = np.random.default_rng()

    n = Q_pr.shape[0]
    m = len(R_diag)

    # é¢„ç¼“å­˜å†³ç­–é˜ˆå€¼
    if hasattr(decision_config, 'tau_iri') and decision_config.tau_iri is not None:
        tau = decision_config.tau_iri
    else:
        tau = decision_config.get_threshold(mu_pr)
        decision_config.tau_iri = tau  # ç¼“å­˜ä»¥é¿å…é‡å¤è®¡ç®—

    # å…ˆéªŒå› å­ï¼ˆç”¨äºæ±‚å¯¹è§’æ–¹å·®ï¼‰
    factor_pr = SparseFactor(Q_pr)

    # é‡‡æ ·æµ‹è¯•ç‚¹ï¼ˆç”¨äºè¯„ä¼°é£é™©ï¼‰
    n_test = min(200, n)
    test_idx = rng.choice(n, size=n_test, replace=False)

    # ğŸ”¥ è®¡ç®—å…ˆéªŒå¯¹è§’æ–¹å·®ï¼ˆåœ¨æµ‹è¯•ç‚¹ä¸Šï¼‰
    var_pr = compute_posterior_variance_diagonal(factor_pr, test_idx)
    sigma_pr = np.sqrt(np.maximum(var_pr, 1e-12))

    # å…ˆéªŒé£é™©ï¼ˆå›ºå®šï¼Œæ‰€æœ‰æ ·æœ¬å…±äº«ï¼‰
    prior_risk = expected_loss(
        mu_pr[test_idx], sigma_pr, decision_config,
        test_indices=np.arange(len(test_idx)), tau=tau
    )

    post_risks = []

    for sample_idx in range(n_samples):
        # === 1. ä»å…ˆéªŒæ­£ç¡®é‡‡æ ·çœŸå®çŠ¶æ€ ===
        x_true = sample_gmrf(Q_pr, mu_pr, rng)

        # === 2. ç”Ÿæˆè§‚æµ‹ y = Hx + Îµ ===
        y_clean = H @ x_true
        noise = rng.normal(0, np.sqrt(R_diag), size=m)
        y = y_clean + noise

        # === 3. è®¡ç®—åéªŒåˆ†å¸ƒ ===
        try:
            mu_post, factor_post = compute_posterior(Q_pr, mu_pr, H, R_diag, y)
        except Exception as e:
            warnings.warn(f"Posterior computation failed at sample {sample_idx}: {e}")
            post_risks.append(prior_risk)
            continue

        # === 4. è®¡ç®—åéªŒå¯¹è§’æ–¹å·®ï¼ˆåœ¨ç›¸åŒæµ‹è¯•ç‚¹ä¸Šï¼‰===
        var_post = compute_posterior_variance_diagonal(factor_post, test_idx)
        sigma_post = np.sqrt(np.maximum(var_post, 1e-12))

        # === 5. è®¡ç®—åéªŒ Bayes é£é™© ===
        post_risk = expected_loss(
            mu_post[test_idx], sigma_post, decision_config,
            test_indices=np.arange(len(test_idx)), tau=tau
        )

        post_risks.append(post_risk)

    # å¹³å‡é£é™©å·®
    avg_post_risk = np.mean(post_risks)
    evi = prior_risk - avg_post_risk

    # ğŸ”¥ å¥åº·æ£€æŸ¥ï¼šEVIåº”è¯¥ä¸ºæ­£
    if evi < -1e-3:  # å…è®¸å°çš„æ•°å€¼è¯¯å·®
        warnings.warn(f"Negative EVI detected: {evi:.2f} Â£")
        warnings.warn(f"  Prior risk: {prior_risk:.2f}, Post risk: {avg_post_risk:.2f}")

    return float(evi)


def evi_unscented(Q_pr, mu_pr, H, R_diag, decision_config,
                       alpha: float = 1.0, beta: float = 2.0,
                       kappa: float = 0.0) -> float:
    """
    ä½¿ç”¨Unscented Transformçš„EVIè¿‘ä¼¼ï¼ˆåœ¨æµ‹é‡ç©ºé—´ï¼‰

    ä¿®å¤ï¼šæ”¹è¿›å…ˆéªŒé£é™©è®¡ç®—
    """
    from inference import compute_posterior, compute_posterior_variance_diagonal, SparseFactor
    import scipy.sparse as sp

    n = Q_pr.shape[0]
    m = len(R_diag)

    # === é¢„æµ‹åˆ†å¸ƒ: y ~ N(H Î¼_pr, H Î£_pr H^T + R) ===
    y_mean = H @ mu_pr

    # è®¡ç®—é¢„æµ‹åæ–¹å·®ï¼ˆå°mæ—¶ç²¾ç¡®ï¼Œå¤§mæ—¶è¿‘ä¼¼ï¼‰
    if m <= 100:
        factor_pr = SparseFactor(Q_pr)
        H_dense = H.toarray() if sp.issparse(H) else H
        Sigma_pr_HT = np.zeros((n, m))
        for i in range(m):
            Sigma_pr_HT[:, i] = factor_pr.solve(H_dense[i, :])
        y_cov = H_dense @ Sigma_pr_HT + np.diag(R_diag)
    else:
        warnings.warn("Large m in UT, using diagonal approximation")
        y_cov = np.diag(R_diag) + 0.1 * np.eye(m)

    # === UTæƒé‡ ===
    lambda_param = alpha ** 2 * (m + kappa) - m
    weights_m = np.full(2 * m + 1, 1.0 / (2 * (m + lambda_param)))
    weights_m[0] = lambda_param / (m + lambda_param)

    # === ç”Ÿæˆsigmaç‚¹ ===
    try:
        L = np.linalg.cholesky(y_cov)
    except np.linalg.LinAlgError:
        L = np.linalg.cholesky(y_cov + 1e-6 * np.eye(m))

    scale = np.sqrt(m + lambda_param)
    sigma_points = [y_mean]

    for i in range(m):
        sigma_points.append(y_mean + scale * L[:, i])
        sigma_points.append(y_mean - scale * L[:, i])

    # === å¯¹æ¯ä¸ªsigmaç‚¹è®¡ç®—åéªŒé£é™© ===
    risks = []
    test_idx = np.linspace(0, n - 1, min(50, n), dtype=int)

    for y_sigma in sigma_points:
        mu_post, factor = compute_posterior(Q_pr, mu_pr, H, R_diag, y_sigma)
        var_post = compute_posterior_variance_diagonal(factor, test_idx)
        sigma_post = np.sqrt(var_post)

        loss = expected_loss(
            mu_post[test_idx],
            sigma_post,
            decision_config,
            test_indices=np.arange(len(test_idx))
        )
        risks.append(loss)

    # åŠ æƒå¹³å‡åéªŒé£é™©
    posterior_risk = np.dot(weights_m, risks)

    # === è®¡ç®—å…ˆéªŒé£é™©ï¼ˆæ”¹è¿›ç‰ˆï¼‰===
    factor_pr = SparseFactor(Q_pr)
    var_pr_sample = compute_posterior_variance_diagonal(factor_pr, test_idx)
    sigma_pr = np.sqrt(var_pr_sample)

    prior_risk = expected_loss(
        mu_pr[test_idx],
        sigma_pr,
        decision_config,
        test_indices=np.arange(len(test_idx))
    )

    evi = prior_risk - posterior_risk
    return float(evi)


# å‘åå…¼å®¹çš„åˆ«å
conditional_risk = conditional_risk
expected_loss = expected_loss
expected_loss_batch = expected_loss_batch
evi_monte_carlo = evi_monte_carlo
evi_unscented = evi_unscented


if __name__ == "__main__":

    from geometry import build_grid2d_geometry
    from spatial_field import build_prior, sample_gmrf
    from sensors import generate_sensor_pool
    from sensors import assemble_H_R

    print("\n" + "=" * 70)
    print("  TESTING FIXED EVI COMPUTATION")
    print("=" * 70)

    from config import load_scenario_config
    cfg = load_scenario_config('baseline_config.yaml')
    rng = cfg.get_rng()

    # Setup
    geom = build_grid2d_geometry(20, 20, h=cfg.geometry.h)
    Q_pr, mu_pr = build_prior(geom, cfg.prior)

    # Generate sensors
    sensors = generate_sensor_pool(geom, cfg.sensors, rng)
    selected = rng.choice(sensors, size=10, replace=False)
    H, R = assemble_H_R(selected, geom.n)

    print("\n[1] Testing unified probability threshold...")

    # ğŸ”¥ æµ‹è¯•ç»Ÿä¸€é˜ˆå€¼å…¬å¼
    test_cases = [
        {'L_FP': 30000, 'L_FN': 120000, 'L_TP': 800, 'L_TN': 0},
        {'L_FP': 5000, 'L_FN': 30000, 'L_TP': 800, 'L_TN': 0},
        {'L_FP': 500, 'L_FN': 10000, 'L_TP': 800, 'L_TN': 100},
    ]

    for i, tc in enumerate(test_cases):
        p_T = get_unified_prob_threshold(tc['L_FP'], tc['L_FN'], tc['L_TP'], tc['L_TN'])
        
        risk = conditional_risk(
            mu=2.0, sigma=0.5, tau=2.2,
            L_FP=tc['L_FP'], L_FN=tc['L_FN'],
            L_TP=tc['L_TP'], L_TN=tc['L_TN']
        )

        print(f"  Case {i+1}: L_FP={tc['L_FP']}, L_FN={tc['L_FN']}")
        print(f"           â†’ p_T={p_T:.3f}, risk=Â£{risk:.2f}")

        # âœ… éªŒè¯é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
        assert 0 <= p_T <= 1, f"Invalid p_T: {p_T}"
        assert 0 <= risk <= max(tc.values()), f"Invalid risk: {risk}"

    print("  âœ… Unified probability threshold correct!")

    print("\n[2] Testing corrected Monte Carlo sampling...")

    # âœ… å…³é”®æµ‹è¯•ï¼šæ£€æŸ¥é‡‡æ ·æ–¹å·®æ˜¯å¦æ­£ç¡®
    from inference import SparseFactor, compute_posterior_variance_diagonal

    factor = SparseFactor(Q_pr)
    test_idx = np.array([100, 200, 300])

    # ç†è®ºæ–¹å·®ï¼ˆä»ç²¾åº¦çŸ©é˜µï¼‰
    var_theory = compute_posterior_variance_diagonal(factor, test_idx)
    print(f"  Theoretical variance: {var_theory}")

    # ç»éªŒæ–¹å·®ï¼ˆä»é‡‡æ ·ï¼‰
    n_samples = 1000
    samples = np.array([sample_gmrf(Q_pr, mu_pr, rng)[test_idx] for _ in range(n_samples)])
    var_empirical = samples.var(axis=0)
    print(f"  Empirical variance:   {var_empirical}")
    print(f"  Relative error:       {np.abs(var_empirical - var_theory) / var_theory}")

    # âœ… å¦‚æœç›¸å¯¹è¯¯å·® < 10%ï¼Œè¯´æ˜é‡‡æ ·æ­£ç¡®
    assert np.all(np.abs(var_empirical - var_theory) / var_theory < 0.15), \
        "âœ— Sampling variance incorrect!"
    print("  âœ… Sampling variance correct!")

    print("\n[3] Testing EVI computation...")

    # Monte Carlo (small sample for speed)
    evi_mc = evi_monte_carlo(Q_pr, mu_pr, H, R, cfg.decision, n_samples=100, rng=rng)
    print(f"  EVI (Monte Carlo, n=100) = Â£{evi_mc:.2f}")

    # âœ… EVI åº”è¯¥ä¸ºæ­£ï¼ˆä¿¡æ¯æ€»æ˜¯æœ‰ä»·å€¼çš„ï¼‰
    assert evi_mc > 0, f"âœ— Negative EVI: {evi_mc:.2f}"
    print(f"  âœ… EVI is positive!")

    print("\n" + "=" * 70)
    print("  ALL TESTS PASSED âœ…")
    print("=" * 70)