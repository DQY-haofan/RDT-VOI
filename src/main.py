"""
ä¸»å®éªŒè„šæœ¬ - å®Œæ•´ç‰ˆï¼ˆæ”¯æŒå‚æ•°æ‰«æå’Œçµæ´»é…ç½®ï¼‰

ğŸ”¥ ä¸»è¦æ”¹è¿›ï¼š
1. ç»Ÿä¸€é…ç½®æ–‡ä»¶ + å‚æ•°è¦†ç›–
2. æ”¯æŒå‚æ•°æ‰«æï¼ˆå•å‚æ•°æˆ–å¤šå‚æ•°ç»„åˆï¼‰
3. ä¿æŒå‘åå…¼å®¹
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„æ ¸å¿ƒå‡½æ•°

ä½¿ç”¨ç¤ºä¾‹ï¼š
# åŸºç¡€ä½¿ç”¨
python main.py                                    # ä½¿ç”¨é»˜è®¤é…ç½®
python main.py --preset high_stakes               # ä½¿ç”¨é«˜é£é™©é¢„è®¾
python main.py --preset low_stakes                # ä½¿ç”¨ä½é£é™©é¢„è®¾

# å•å‚æ•°è°ƒæ•´
python main.py --ddi 0.30 --fn-cost 120000        # å¿«é€Ÿè°ƒæ•´å…³é”®å‚æ•°
python main.py --grid-size 25 --budgets 5,10,15   # è°ƒæ•´å®éªŒè§„æ¨¡

# å‚æ•°æ‰«æ
python main.py --scan ddi=0.1,0.2,0.3             # DDIæ‰«æ
python main.py --scan fn_cost=30000,60000,120000   # æˆæœ¬æ‰«æ
python main.py --scan ddi=0.2,0.3 fn_cost=60000,120000  # ç»„åˆæ‰«æ

# æ§åˆ¶é€‰é¡¹
python main.py --parallel --workers 6             # å¹¶è¡Œå¤„ç†
python main.py --quick-test                       # å¿«é€Ÿæµ‹è¯•
"""

from pathlib import Path
from datetime import datetime
import json
import pickle
import sys
import warnings
import numpy as np
import pandas as pd
import argparse
from matplotlib import pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
import time
import itertools
import scipy.sparse as sp
# ğŸ”¥ æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))


from config import load_config, generate_parameter_combinations, parse_scan_parameter
from geometry import build_grid2d_geometry
from spatial_field import build_prior, sample_gmrf, build_prior_with_ddi
from sensors import generate_sensor_pool
from inference import compute_posterior, compute_posterior_variance_diagonal, SparseFactor
from sensors import get_observation

from method_wrappers import get_selection_method, get_available_methods
from evaluation import spatial_block_cv, compute_metrics, morans_i

from visualization import (
    setup_style,
    generate_all_visualizations_v2,
    aggregate_results_for_visualization
)


class NumpyEncoder(json.JSONEncoder):
    """å¤„ç†numpyç±»å‹çš„JSON encoder"""
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)


# ============================================================================
# å‘½ä»¤è¡Œå‚æ•°è§£æ
# ============================================================================

def parse_arguments():
    """
    ğŸ”¥ å¢å¼ºçš„å‘½ä»¤è¡Œå‚æ•°è§£æ - æ”¯æŒå‚æ•°æ‰«æ
    """
    parser = argparse.ArgumentParser(
        description='RDT-VoI å‚æ•°åŒ–å®éªŒæ¡†æ¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# åŸºç¡€ä½¿ç”¨
  python main.py                               # ä½¿ç”¨é»˜è®¤é…ç½®
  python main.py --preset high_stakes          # é«˜é£é™©åœºæ™¯
  python main.py --preset low_stakes           # ä½é£é™©åœºæ™¯

# å•å‚æ•°è°ƒæ•´  
  python main.py --ddi 0.30                    # è°ƒæ•´DDI
  python main.py --fn-cost 120000              # è°ƒæ•´è¯¯æ£€æˆæœ¬
  python main.py --grid-size 25                # è°ƒæ•´ç½‘æ ¼å¤§å°
  python main.py --budgets 5,10,15,20          # è°ƒæ•´é¢„ç®—åˆ—è¡¨

# å‚æ•°æ‰«æ
  python main.py --scan ddi=0.1,0.2,0.3                    # DDIæ‰«æ
  python main.py --scan fn_cost=30000,60000,120000         # æˆæœ¬æ‰«æ  
  python main.py --scan ddi=0.2,0.3 fn_cost=60000,120000  # ç»„åˆæ‰«æ

# é«˜çº§é€‰é¡¹
  python main.py --parallel --workers 6        # å¹¶è¡Œå¤„ç†
  python main.py --quick-test                  # å¿«é€Ÿæµ‹è¯•
  python main.py --dry-run                     # é¢„è§ˆå‚æ•°ç»„åˆ
        """
    )

    # æ ¸å¿ƒé…ç½®é€‰é¡¹
    config_group = parser.add_argument_group('é…ç½®é€‰é¡¹')
    config_group.add_argument(
        '--config', '-c', type=str, default='baseline_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: baseline_config.yaml)'
    )
    config_group.add_argument(
        '--preset', '-p', type=str, choices=['high_stakes', 'low_stakes'],
        help='é¢„è®¾åœºæ™¯ (high_stakes=é«˜é£é™©, low_stakes=ä½é£é™©)'
    )

    # ğŸ”¥ å…³é”®å‚æ•°å¿«é€Ÿè°ƒæ•´
    param_group = parser.add_argument_group('å…³é”®å‚æ•°è°ƒæ•´')
    param_group.add_argument(
        '--ddi', type=float,
        help='å†³ç­–éš¾åº¦æŒ‡æ•° (0.0-1.0, å…¸å‹å€¼: 0.10-0.30)'
    )
    param_group.add_argument(
        '--fn-cost', '--fn_cost', type=float,
        help='è¯¯æ£€æˆæœ¬ (Â£, å…¸å‹å€¼: 30000-120000)'
    )
    param_group.add_argument(
        '--fp-cost', '--fp_cost', type=float,
        help='è¯¯æŠ¥æˆæœ¬ (Â£, å…¸å‹å€¼: 5000-30000)'
    )
    param_group.add_argument(
        '--tau-quantile', '--tau_quantile', type=float,
        help='é˜ˆå€¼åˆ†ä½æ•° (0.0-1.0, å…¸å‹å€¼: 0.65-0.88)'
    )
    param_group.add_argument(
        '--action-limit', '--K_action', type=int,
        help='è¡ŒåŠ¨é™åˆ¶ (æ•´æ•°, null=æ— é™åˆ¶)'
    )

    # å®éªŒè§„æ¨¡è°ƒæ•´
    scale_group = parser.add_argument_group('å®éªŒè§„æ¨¡')
    scale_group.add_argument(
        '--grid-size', '--grid_size', type=int,
        help='ç½‘æ ¼å¤§å° (nx=ny, å…¸å‹å€¼: 15-25)'
    )
    scale_group.add_argument(
        '--budgets', type=str,
        help='é¢„ç®—åˆ—è¡¨ (é€—å·åˆ†éš”, å¦‚: 5,10,15,20)'
    )
    scale_group.add_argument(
        '--methods', type=str,
        help='æ–¹æ³•åˆ—è¡¨ (é€—å·åˆ†éš”, å¦‚: greedy_mi,greedy_evi,uniform)'
    )
    scale_group.add_argument(
        '--folds', '--k_folds', type=int,
        help='äº¤å‰éªŒè¯æŠ˜æ•° (å…¸å‹å€¼: 3-10)'
    )

    # ğŸ”¥ å‚æ•°æ‰«æåŠŸèƒ½
    scan_group = parser.add_argument_group('å‚æ•°æ‰«æ')
    scan_group.add_argument(
        '--scan', type=str, nargs='+',
        help='å‚æ•°æ‰«æ (æ ¼å¼: param=val1,val2,val3). ä¾‹å¦‚: --scan ddi=0.1,0.2,0.3 fn_cost=30000,60000'
    )
    scan_group.add_argument(
        '--scan-presets', type=str, nargs='+',
        help='é¢„è®¾æ‰«æ (å¦‚: --scan-presets high_stakes low_stakes)'
    )
    scan_group.add_argument(
        '--dry-run', action='store_true',
        help='ä»…æ˜¾ç¤ºå‚æ•°ç»„åˆï¼Œä¸æ‰§è¡Œå®éªŒ'
    )

    # æ‰§è¡Œæ§åˆ¶
    exec_group = parser.add_argument_group('æ‰§è¡Œæ§åˆ¶')
    exec_group.add_argument(
        '--parallel', action='store_true',
        help='å¯ç”¨å¹¶è¡Œå¤„ç†'
    )
    exec_group.add_argument(
        '--workers', type=int, default=None,
        help='å¹¶è¡Œworkeræ•°é‡ (é»˜è®¤: CPUæ ¸å¿ƒæ•°-1)'
    )
    exec_group.add_argument(
        '--output', '-o', type=str, default=None,
        help='è¾“å‡ºç›®å½• (é»˜è®¤: ä»é…ç½®è¯»å–)'
    )

    # è°ƒè¯•å’Œæµ‹è¯•
    debug_group = parser.add_argument_group('è°ƒè¯•å’Œæµ‹è¯•')
    debug_group.add_argument(
        '--quick-test', action='store_true',
        help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (å°ç½‘æ ¼ï¼Œå°‘é¢„ç®—ï¼Œå°‘fold)'
    )
    debug_group.add_argument(
        '--skip-viz', action='store_true',
        help='è·³è¿‡å¯è§†åŒ–ç”Ÿæˆ'
    )
    debug_group.add_argument(
        '--seed', type=int, default=None,
        help='éšæœºç§å­è¦†ç›–'
    )
    debug_group.add_argument(
        '-v', '--verbose', action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    debug_group.add_argument(
        '-q', '--quiet', action='store_true',
        help='å®‰é™æ¨¡å¼'
    )

    return parser.parse_args()


# ============================================================================
# é…ç½®å¤„ç†å‡½æ•°
# ============================================================================

def apply_cli_overrides(cfg, args):
    """
    ğŸ”¥ åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–åˆ°é…ç½®
    """
    overrides = {}

    # æ”¶é›†æ‰€æœ‰éç©ºçš„CLIå‚æ•°
    cli_mappings = {
        'ddi': 'target_ddi',
        'fn_cost': 'L_FN_gbp',
        'fp_cost': 'L_FP_gbp',
        'tau_quantile': 'tau_quantile',
        'action_limit': 'K_action',
        'grid_size': 'grid_size',
        'budgets': 'budgets',
        'methods': 'methods',
        'folds': 'k_folds',
        'seed': 'seed'
    }

    for cli_arg, config_key in cli_mappings.items():
        value = getattr(args, cli_arg, None)
        if value is not None:
            overrides[config_key] = value

    # åº”ç”¨è¦†ç›–
    if overrides:
        if not args.quiet:
            print(f"\nğŸ“ Applying CLI overrides: {overrides}")
        cfg = cfg.apply_parameter_overrides(overrides, verbose=not args.quiet)

    return cfg


def parse_scan_parameters(scan_args):
    """
    ğŸ”¥ è§£ææ‰«æå‚æ•°

    Args:
        scan_args: ['ddi=0.1,0.2,0.3', 'fn_cost=30000,60000']

    Returns:
        {'ddi': [0.1, 0.2, 0.3], 'fn_cost': [30000, 60000]}
    """
    scan_params = {}

    for scan_spec in scan_args:
        if '=' not in scan_spec:
            raise ValueError(f"Invalid scan format: {scan_spec}. Use param=val1,val2,val3")

        param_name, values_str = scan_spec.split('=', 1)
        param_name = param_name.strip()

        # è§£æå€¼åˆ—è¡¨
        values = parse_scan_parameter(values_str)
        if not values:
            raise ValueError(f"No values found for parameter: {param_name}")

        scan_params[param_name] = values

    return scan_params


def create_experiment_configs(base_cfg, args):
    """
    ğŸ”¥ åˆ›å»ºå®éªŒé…ç½®åˆ—è¡¨ï¼ˆæ”¯æŒå‚æ•°æ‰«æï¼‰
    """
    configs = []

    # æƒ…å†µ1: é¢„è®¾æ‰«æ
    if args.scan_presets:
        print(f"\nğŸ” Preset scanning: {args.scan_presets}")
        for preset_name in args.scan_presets:
            try:
                preset_cfg = base_cfg.apply_preset(preset_name, verbose=not args.quiet)
                preset_cfg.experiment.name = f"{base_cfg.experiment.name}_{preset_name}"
                configs.append(preset_cfg)
            except Exception as e:
                print(f"âŒ Failed to apply preset {preset_name}: {e}")
                continue

    # æƒ…å†µ2: å‚æ•°æ‰«æ
    elif args.scan:
        print(f"\nğŸ” Parameter scanning: {args.scan}")
        scan_params = parse_scan_parameters(args.scan)
        combinations = generate_parameter_combinations(scan_params)

        print(f"ğŸ“Š Generated {len(combinations)} parameter combinations")

        for i, combo in enumerate(combinations):
            combo_cfg = base_cfg.apply_parameter_overrides(combo, verbose=False)

            # ç”Ÿæˆæè¿°æ€§åç§°
            combo_desc = "_".join(f"{k}{v}" for k, v in list(combo.items())[:3])  # é™åˆ¶é•¿åº¦
            combo_cfg.experiment.name = f"{base_cfg.experiment.name}_scan_{combo_desc}"

            if not args.quiet:
                print(f"  {i+1}: {combo}")

            configs.append(combo_cfg)

    # æƒ…å†µ3: å•ä¸€é…ç½®
    else:
        configs.append(base_cfg)

    return configs


def detect_scenario_from_config(cfg) -> str:
    """ä»é…ç½®è‡ªåŠ¨æ£€æµ‹åœºæ™¯ç±»å‹"""
    exp_name = cfg.experiment.name.lower()
    if 'high' in exp_name or 'stakes' in exp_name:
        return 'A'
    elif 'low' in exp_name or 'proxy' in exp_name:
        return 'B'

    ddi = getattr(cfg.decision, 'target_ddi', 0.0)
    fn_fp_ratio = cfg.decision.L_FN_gbp / cfg.decision.L_FP_gbp if cfg.decision.L_FP_gbp > 0 else 1.0

    if ddi >= 0.2 or fn_fp_ratio > 10:
        return 'A'
    elif ddi < 0.15 or fn_fp_ratio < 5:
        return 'B'

    return 'A'  # é»˜è®¤


def create_output_dir_from_config(cfg, config_path: str, custom_output: str = None) -> Path:
    """æ ¹æ®é…ç½®æ–‡ä»¶ååˆ›å»ºè¾“å‡ºç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä»é…ç½®æ–‡ä»¶åæå–åœºæ™¯æ ‡è¯†
    config_name = Path(config_path).stem
    if config_name.startswith('config_'):
        scenario_name = config_name[7:]
    else:
        scenario_name = config_name

    # ç¡®å®šè¾“å‡ºæ ¹ç›®å½•
    if custom_output:
        base_dir = Path(custom_output)
    else:
        base_dir = Path(cfg.experiment.output_dir)

    # åˆ›å»ºå±‚çº§ç›®å½•ç»“æ„
    output_dir = base_dir / f"exp_{cfg.experiment.name}" / f"run_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºå­ç›®å½•
    (output_dir / "plots").mkdir(exist_ok=True)

    # ä¿å­˜é…ç½®å‰¯æœ¬
    cfg.save_to(output_dir)

    # ä¿å­˜è¿è¡Œå‘½ä»¤
    with open(output_dir / "run_command.txt", 'w', encoding='utf-8') as f:
        f.write(' '.join(sys.argv))

    return output_dir


def apply_quick_test_overrides(cfg):
    """åº”ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼çš„è¦†ç›–"""
    print(f"\nğŸš€ Quick test mode enabled:")

    # å°ç½‘æ ¼
    cfg.geometry.nx = 10
    cfg.geometry.ny = 10
    print(f"    â†’ Grid: {cfg.geometry.nx}Ã—{cfg.geometry.ny}")

    # å°‘é¢„ç®—
    cfg.selection.budgets = [3, 5]
    print(f"    â†’ Budgets: {cfg.selection.budgets}")

    # å°‘fold
    cfg.cv.k_folds = 2
    print(f"    â†’ CV folds: {cfg.cv.k_folds}")

    # å‡å°‘é‡‡æ ·
    cfg.evi.monte_carlo_samples = 8
    print(f"    â†’ MC samples: {cfg.evi.monte_carlo_samples}")

    return cfg


# ============================================================================
# ğŸ”¥ æ ¸å¿ƒå‡½æ•°1: run_single_fold_worker (å®Œæ•´ç‰ˆ)
# ============================================================================

def run_single_fold_worker(fold_data: dict) -> dict:
    """
    âœ… å®Œå…¨ä¿®å¤ç‰ˆï¼šé¿å…geomå¯¹è±¡åºåˆ—åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
    """
    import time
    import warnings
    import numpy as np
    import scipy.sparse as sp
    from inference import compute_posterior, compute_posterior_variance_diagonal, SparseFactor
    from sensors import get_observation
    from evaluation import compute_metrics, morans_i
    from decision import expected_loss

    # è§£åŒ…æ•°æ®
    train_idx = fold_data['train_idx']
    test_idx = fold_data['test_idx']
    selection_method = fold_data['selection_method']
    k = fold_data['k']
    Q_pr = fold_data['Q_pr']
    mu_pr = fold_data['mu_pr']
    x_true = fold_data['x_true']
    sensors = fold_data['sensors']
    decision_config = fold_data['decision_config']

    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æ ‡é‡ï¼Œä¸æ„å»ºgeomå¯¹è±¡
    n_domain = fold_data['n_domain']
    coords = fold_data['coords']
    adjacency_test_data = fold_data.get('adjacency_test')

    rng = np.random.default_rng(fold_data['rng_seed'])

    # ä»configè¯»å–æ˜¯å¦å¯ç”¨domain scaling
    enable_scaling = fold_data.get('enable_domain_scaling', True)

    # æ£€æµ‹åœºæ™¯ç±»å‹
    scenario = fold_data.get('scenario', 'A')

    morans_permutations = fold_data.get('morans_permutations', 999)

    try:
        # ====================================================================
        # 1. è®¡ç®—å…ˆéªŒæŸå¤±ï¼ˆç”¨äºROIï¼‰
        # ====================================================================
        t_prior_start = time.time()
        tau = decision_config.get_threshold(mu_pr)
        factor_pr = SparseFactor(Q_pr)
        var_pr_test = compute_posterior_variance_diagonal(factor_pr, test_idx)
        sigma_pr_test = np.sqrt(np.maximum(var_pr_test, 1e-12))

        prior_loss_test = expected_loss(
            mu_pr[test_idx], sigma_pr_test, decision_config,
            test_indices=np.arange(len(test_idx)), tau=tau
        )

        # ğŸ”¥ Domain Scaling
        if enable_scaling:
            N_test = len(test_idx)
            scale_factor = n_domain / N_test
            prior_loss_scaled = prior_loss_test * scale_factor
        else:
            prior_loss_scaled = prior_loss_test
            scale_factor = 1.0

        prior_time = time.time() - t_prior_start

        # ====================================================================
        # 2. ä¼ æ„Ÿå™¨é€‰æ‹©
        # ====================================================================
        t_sel_start = time.time()
        selection_result = selection_method(sensors, k, Q_pr, mu_pr)
        selection_time = time.time() - t_sel_start

        selected_sensors = [sensors[i] for i in selection_result.selected_ids]
        sensor_cost = selection_result.total_cost

        # ====================================================================
        # 3. ç”Ÿæˆè§‚æµ‹ + è®¡ç®—åéªŒ
        # ====================================================================
        y, H, R = get_observation(x_true, selected_sensors, rng)

        t_inf_start = time.time()
        mu_post, factor_post = compute_posterior(Q_pr, mu_pr, H, R, y)
        inference_time = time.time() - t_inf_start

        var_post_test = compute_posterior_variance_diagonal(factor_post, test_idx)
        sigma_post_test = np.sqrt(np.maximum(var_post_test, 1e-12))
        sigma_post = np.zeros(len(mu_post))
        sigma_post[test_idx] = sigma_post_test

        # ====================================================================
        # 4. åŸºç¡€æŒ‡æ ‡
        # ====================================================================
        metrics = compute_metrics(mu_post, sigma_post, x_true, test_idx, decision_config)
        posterior_loss_test = metrics['expected_loss_gbp']

        if enable_scaling:
            posterior_loss_scaled = posterior_loss_test * scale_factor
        else:
            posterior_loss_scaled = posterior_loss_test

        # ROIè®¡ç®—
        savings_scaled = prior_loss_scaled - posterior_loss_scaled

        if sensor_cost > 0:
            roi = (savings_scaled - sensor_cost) / sensor_cost
            cost_efficiency = savings_scaled / sensor_cost
        else:
            roi = np.inf if savings_scaled > 0 else 0.0
            cost_efficiency = np.inf if savings_scaled > 0 else 0.0

        # ====================================================================
        # 5. Scenario A ç‰¹æœ‰ï¼šNear-threshold å­é›†è¯„ä¼°
        # ====================================================================
        near_threshold_metrics = {}
        if scenario == 'A':
            try:
                gaps_prior = np.abs(mu_pr[test_idx] - tau)
                near_mask = gaps_prior <= 1.0 * sigma_pr_test

                if near_mask.sum() > 0:
                    prior_loss_near = expected_loss(
                        mu_pr[test_idx][near_mask],
                        sigma_pr_test[near_mask],
                        decision_config,
                        test_indices=np.arange(near_mask.sum()),
                        tau=tau
                    )

                    posterior_loss_near = expected_loss(
                        mu_post[test_idx][near_mask],
                        sigma_post_test[near_mask],
                        decision_config,
                        test_indices=np.arange(near_mask.sum()),
                        tau=tau
                    )

                    if enable_scaling:
                        prior_loss_near *= scale_factor
                        posterior_loss_near *= scale_factor

                    savings_near = prior_loss_near - posterior_loss_near
                    roi_near = (savings_near - sensor_cost) / sensor_cost if sensor_cost > 0 else 0.0

                    near_threshold_metrics = {
                        'n_near_threshold': int(near_mask.sum()),
                        'fraction_near_threshold': float(near_mask.sum() / len(test_idx)),
                        'prior_loss_near_threshold': float(prior_loss_near),
                        'posterior_loss_near_threshold': float(posterior_loss_near),
                        'savings_near_threshold': float(savings_near),
                        'roi_near_threshold': float(roi_near)
                    }
            except Exception as e:
                warnings.warn(f"Near-threshold evaluation failed: {e}")

        # è®°å½•å®Œæ•´æŒ‡æ ‡
        metrics.update({
            'roi': float(roi),
            'cost_efficiency': float(cost_efficiency),
            'prior_loss_gbp': float(prior_loss_scaled),
            'posterior_loss_gbp': float(posterior_loss_scaled),
            'savings_gbp': float(savings_scaled),
            'total_cost': float(sensor_cost),
            'prior_loss_test_only': float(prior_loss_test),
            'domain_scale_factor': float(scale_factor),
            **near_threshold_metrics
        })

        # ====================================================================
        # 6. DDIç»Ÿè®¡
        # ====================================================================
        try:
            from spatial_field import compute_ddi
            ddi_test = compute_ddi(mu_post[test_idx], sigma_post_test, tau, k=1.0)
            metrics['ddi_test'] = float(ddi_test)

            sample_size = min(200, len(mu_pr))
            sample_idx = rng.choice(len(mu_pr), size=sample_size, replace=False)
            var_pr_sample = compute_posterior_variance_diagonal(factor_pr, sample_idx)
            sigma_pr_sample = np.sqrt(np.maximum(var_pr_sample, 1e-12))
            ddi_prior = compute_ddi(mu_pr[sample_idx], sigma_pr_sample, tau, k=1.0)
            metrics['ddi_prior'] = float(ddi_prior)
        except Exception as e:
            warnings.warn(f"DDI computation failed: {e}")
            metrics['ddi_test'] = np.nan
            metrics['ddi_prior'] = np.nan

        # ====================================================================
        # 7. Scenario A ç‰¹æœ‰ï¼šè¡ŒåŠ¨å—é™è¯„ä¼°
        # ====================================================================
        if scenario == 'A' and hasattr(decision_config, 'K_action') and decision_config.K_action is not None:
            try:
                from scipy.stats import norm
                K_action = decision_config.K_action
                p_failure = 1.0 - norm.cdf((tau - mu_post[test_idx]) / np.maximum(sigma_post_test, 1e-12))

                if K_action < len(test_idx):
                    top_k_local = np.argsort(p_failure)[-K_action:]
                else:
                    top_k_local = np.arange(len(test_idx))

                constrained_risks = np.zeros(len(test_idx))
                for i in range(len(test_idx)):
                    global_idx = test_idx[i]
                    if i in top_k_local:
                        if x_true[global_idx] > tau:
                            constrained_risks[i] = decision_config.L_TP_gbp
                        else:
                            constrained_risks[i] = decision_config.L_FP_gbp
                    else:
                        if x_true[global_idx] > tau:
                            constrained_risks[i] = decision_config.L_FN_gbp
                        else:
                            constrained_risks[i] = decision_config.L_TN_gbp

                constrained_loss = constrained_risks.mean()
                true_exceed = x_true[test_idx] > tau
                if true_exceed.sum() > 0:
                    hit_rate = np.sum(np.isin(top_k_local, np.where(true_exceed)[0])) / true_exceed.sum()
                else:
                    hit_rate = 1.0

                metrics['action_K'] = int(K_action)
                metrics['action_constrained_loss'] = float(constrained_loss)
                metrics['action_regret'] = float(constrained_loss - posterior_loss_test)
                metrics['action_hit_rate'] = float(hit_rate)
            except Exception as e:
                warnings.warn(f"Action-constrained evaluation failed: {e}")

        # ====================================================================
        # 8. Moran's I
        # ====================================================================
        residuals = mu_post - x_true
        if adjacency_test_data is not None:
            try:
                adj_test = sp.coo_matrix(
                    (adjacency_test_data['data'],
                     (adjacency_test_data['row'], adjacency_test_data['col'])),
                    shape=adjacency_test_data['shape']
                ).tocsr()

                I_stat, I_pval = morans_i(
                    residuals[test_idx],
                    adj_test,
                    n_permutations=morans_permutations,
                    rng=rng
                )
                metrics['morans_i'] = float(I_stat)
                metrics['morans_pval'] = float(I_pval)
            except Exception as e:
                warnings.warn(f"Moran's I computation failed: {e}")

        # ====================================================================
        # 9. æ—¶é—´ç»Ÿè®¡
        # ====================================================================
        metrics['prior_computation_time_sec'] = float(prior_time)
        metrics['selection_time_sec'] = float(selection_time)
        metrics['inference_time_sec'] = float(inference_time)
        metrics['total_time_sec'] = float(prior_time + selection_time + inference_time)

        # ====================================================================
        # 10. ä¼ æ„Ÿå™¨è¯Šæ–­
        # ====================================================================
        metrics['n_selected'] = len(selection_result.selected_ids)

        type_counts = {}
        for sid in selection_result.selected_ids:
            stype = sensors[sid].type_name
            type_counts[stype] = type_counts.get(stype, 0) + 1
        metrics['type_counts'] = {k: int(v) for k, v in type_counts.items()}

        selected_costs = [sensors[i].cost for i in selection_result.selected_ids]
        metrics['cost_mean'] = float(np.mean(selected_costs))
        metrics['cost_std'] = float(np.std(selected_costs))

        if 'coverage_90' in metrics:
            metrics['coverage_90'] = float(np.clip(metrics['coverage_90'], 0.0, 1.0))

        return {
            'success': True,
            'metrics': metrics,
            'selection_result': selection_result,
            'mu_post': mu_post,
            'sigma_post': sigma_post,
            'residuals': mu_post[test_idx] - x_true[test_idx],
            'test_idx': test_idx,
            'tau': tau,
            'prior_loss': prior_loss_scaled,
            'posterior_loss': posterior_loss_scaled,
            'savings': savings_scaled,
            'roi': roi
        }

    except Exception as e:
        import traceback
        return {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
        }


# ============================================================================
# ğŸ”¥ æ ¸å¿ƒå‡½æ•°2: run_method_evaluation (å®Œæ•´ç‰ˆ)
# ============================================================================

def run_method_evaluation(method_name: str, cfg, geom, Q_pr, mu_pr,
                          x_true, sensors, test_idx_global=None,
                          use_parallel=False, n_workers=None, verbose=True) -> dict:
    """
    è¿è¡Œæ–¹æ³•è¯„ä¼°ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰
    """
    if verbose:
        print(f"\n{'=' * 70}")
        print(f"  Method: {method_name.upper()}")
        if use_parallel:
            print(f"  Mode: PARALLEL ({n_workers or 'auto'} workers)")
        else:
            print(f"  Mode: SEQUENTIAL")
        print(f"{'=' * 70}")

    rng = cfg.get_rng()

    # åˆ›å»ºé€‰æ‹©æ–¹æ³•wrapper
    try:
        selection_method = get_selection_method(
            method_name=method_name,
            config=cfg,
            geom=geom,
            x_true=x_true,
            test_idx=test_idx_global
        )
    except Exception as e:
        if verbose:
            print(f"  âœ— Failed to create method wrapper: {e}")
        raise

    # ç”ŸæˆCV folds
    buffer_width = cfg.cv.buffer_width_multiplier * cfg.prior.correlation_length
    folds = spatial_block_cv(
        geom.coords, cfg.cv.k_folds, buffer_width,
        cfg.cv.block_strategy, rng
    )

    if verbose:
        for fold_idx, (train_idx, test_idx) in enumerate(folds):
            print(f"  Fold {fold_idx + 1}: train={len(train_idx)}, test={len(test_idx)}")

    results = {
        'budgets': {},
        'method_name': method_name,
        'n_folds': len(folds)
    }

    # æ£€æµ‹åœºæ™¯ç±»å‹
    if hasattr(cfg.decision, 'target_ddi'):
        if cfg.decision.target_ddi >= 0.20:
            scenario = 'A'
        else:
            scenario = 'B'
    else:
        scenario = 'A'

    enable_scaling = getattr(cfg.metrics, 'scale_savings_to_domain', True) if hasattr(cfg, 'metrics') else True

    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæå–å¿…è¦çš„æ ‡é‡å’Œå¯åºåˆ—åŒ–æ•°æ®
    n_domain = geom.n
    coords = geom.coords

    # éå†budgets
    for k in cfg.selection.budgets:
        if verbose:
            print(f"\n  Budget k={k}")
            print(f"  {'-' * 50}")

        budget_results = {
            'fold_results': [],
            'fold_metrics': []
        }

        # ğŸ”¥ å‡†å¤‡æ‰€æœ‰foldçš„æ•°æ®
        fold_data_list = []
        for fold_idx, (train_idx, test_idx) in enumerate(folds):
            # ğŸ”¥ å…³é”®ï¼šæå–test_idxå¯¹åº”çš„adjacencyå­çŸ©é˜µ
            try:
                adj_test_submatrix = geom.adjacency[test_idx][:, test_idx]
                adj_test_coo = adj_test_submatrix.tocoo()
                adjacency_data = {
                    'data': adj_test_coo.data,
                    'row': adj_test_coo.row,
                    'col': adj_test_coo.col,
                    'shape': adj_test_coo.shape
                }
            except Exception as e:
                if verbose:
                    print(f"    Warning: Failed to extract adjacency submatrix: {e}")
                adjacency_data = None

            fold_data = {
                'train_idx': train_idx,
                'test_idx': test_idx,
                'selection_method': selection_method,
                'k': k,
                'Q_pr': Q_pr,
                'mu_pr': mu_pr,
                'x_true': x_true,
                'sensors': sensors,
                'decision_config': cfg.decision,
                'n_domain': n_domain,
                'coords': coords,
                'adjacency_test': adjacency_data,
                'rng_seed': rng.integers(0, 2 ** 31),
                'enable_domain_scaling': enable_scaling,
                'scenario': scenario,
                'morans_permutations': cfg.cv.morans_permutations if hasattr(cfg.cv, 'morans_permutations') else 999,
                'verbose': verbose
            }
            fold_data_list.append((fold_idx, fold_data))

        # å¹¶è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œ
        if use_parallel and len(fold_data_list) > 1:
            if n_workers is None:
                n_workers = max(1, mp.cpu_count() - 1)

            if verbose:
                print(f"    Running {len(fold_data_list)} folds in parallel "
                      f"with {n_workers} workers...")

            with ProcessPoolExecutor(max_workers=n_workers) as executor:
                future_to_fold = {
                    executor.submit(run_single_fold_worker, fold_data): fold_idx
                    for fold_idx, fold_data in fold_data_list
                }

                for future in as_completed(future_to_fold):
                    fold_idx = future_to_fold[future]
                    try:
                        fold_result = future.result()
                        budget_results['fold_results'].append(fold_result)

                        if fold_result['success']:
                            metrics = fold_result['metrics']
                            budget_results['fold_metrics'].append(metrics)
                            if verbose:
                                print(f"    Fold {fold_idx + 1}: "
                                      f"RMSE={metrics['rmse']:.3f}, "
                                      f"Loss=Â£{metrics['expected_loss_gbp']:.0f}")
                        else:
                            if verbose:
                                print(f"    Fold {fold_idx + 1}: "
                                      f"âœ— {fold_result.get('error', 'unknown')}")
                    except Exception as e:
                        if verbose:
                            print(f"    Fold {fold_idx + 1}: âœ— Exception: {e}")
                        budget_results['fold_results'].append({
                            'success': False,
                            'error': str(e)
                        })
        else:
            # ä¸²è¡Œæ¨¡å¼
            for fold_idx, fold_data in fold_data_list:
                if verbose:
                    print(f"    Fold {fold_idx + 1}/{len(folds)}: "
                          f"train={len(fold_data['train_idx'])}, "
                          f"test={len(fold_data['test_idx'])}")

                try:
                    fold_result = run_single_fold_worker(fold_data)
                    budget_results['fold_results'].append(fold_result)

                    if fold_result['success']:
                        metrics = fold_result['metrics']
                        budget_results['fold_metrics'].append(metrics)
                        if verbose:
                            print(f"        RMSE={metrics['rmse']:.3f}, "
                                  f"Loss=Â£{metrics['expected_loss_gbp']:.0f}, "
                                  f"Coverage={metrics['coverage_90'] * 100:.2f}%")
                    else:
                        if verbose:
                            print(f"        âœ— FAILED: "
                                  f"{fold_result.get('error', 'unknown error')}")
                except Exception as e:
                    if verbose:
                        print(f"        âœ— Exception: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    budget_results['fold_results'].append({
                        'success': False,
                        'error': str(e),
                        'traceback': traceback.format_exc()
                    })

        # è®¡ç®—budgetçº§åˆ«çš„ç»Ÿè®¡
        if budget_results['fold_metrics']:
            n_folds = len(budget_results['fold_metrics'])
            aggregated = {}

            for key in budget_results['fold_metrics'][0].keys():
                if key in ['z_scores', 'n_test', 'n_selected', 'type_counts']:
                    continue

                values = [m[key] for m in budget_results['fold_metrics'] if key in m]
                if values and all(isinstance(v, (int, float)) for v in values):
                    aggregated[key] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'values': values
                    }

            budget_results['aggregated'] = aggregated

            if verbose:
                print(f"\n    Summary (n={n_folds} folds):")
                for metric in ['expected_loss_gbp', 'rmse']:
                    if metric in aggregated:
                        stats = aggregated[metric]
                        if 'loss' in metric:
                            mean_str = f"{stats['mean']:.0f}"
                            std_str = f"{stats['std']:.0f}"
                        else:
                            mean_str = f"{stats['mean']:.3f}"
                            std_str = f"{stats['std']:.3f}"
                        print(f"      {metric.replace('_', ' ').title()}: "
                              f"{mean_str} Â± {std_str}")
        else:
            if verbose:
                print(f"\n    âš ï¸  No successful folds for budget k={k}")

        results['budgets'][k] = budget_results

    return results


# ============================================================================
# ğŸ”¥ æ ¸å¿ƒå‡½æ•°3: run_single_experiment
# ============================================================================

def run_single_experiment(cfg, args, exp_index=None, total_experiments=None):
    """
    ğŸ”¥ è¿è¡Œå•ä¸ªå®éªŒé…ç½®ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰

    ä¿®å¤è¦ç‚¹ï¼š
    1. å…ˆåº”ç”¨DDIæ§åˆ¶ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œç„¶åé”å®šé˜ˆå€¼
    2. åŸºäºæœ€ç»ˆçš„å…ˆéªŒåˆ†å¸ƒé”å®šé˜ˆå€¼ï¼Œè€Œä¸æ˜¯åˆå§‹å…ˆéªŒ
    3. å®Œå–„çš„å¼‚å¸¸æ£€æµ‹å’Œè¯Šæ–­
    """
    exp_prefix = f"[{exp_index + 1}/{total_experiments}] " if exp_index is not None else ""

    if not args.quiet:
        print(f"\n{'=' * 70}")
        print(f"  {exp_prefix}EXPERIMENT: {cfg.experiment.name}")
        print(f"{'=' * 70}")

    t_start = datetime.now()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_output_dir_from_config(
        cfg,
        args.config or "baseline_config.yaml",
        args.output
    )
    if not args.quiet:
        print(f"\nğŸ“ Output: {output_dir}")

    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼è°ƒæ•´
    if args.quick_test:
        cfg = apply_quick_test_overrides(cfg)

    rng = cfg.get_rng()

    # ========================================================================
    # ğŸ”¥ ã€æ ¸å¿ƒä¿®å¤ã€‘æ„å»ºåŸŸå’Œå…ˆéªŒçš„æ­£ç¡®é¡ºåº
    # ========================================================================

    if not args.quiet:
        print(f"\nğŸŒ Building domain: {cfg.geometry.nx}Ã—{cfg.geometry.ny}")
    geom = build_grid2d_geometry(cfg.geometry.nx, cfg.geometry.ny, cfg.geometry.h)

    if not args.quiet:
        print(f"ğŸ”§ Building prior...")

    # æ­¥éª¤1ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦DDIæ§åˆ¶
    use_ddi = (hasattr(cfg.decision, 'target_ddi') and
               cfg.decision.target_ddi is not None and
               cfg.decision.target_ddi > 0)

    if use_ddi:
        # ====================================================================
        # ğŸ”¥ æƒ…å†µAï¼šéœ€è¦DDIæ§åˆ¶
        # é¡ºåºï¼šæ„å»ºåˆå§‹å…ˆéªŒ â†’ DDIæ§åˆ¶ â†’ é”å®šé˜ˆå€¼
        # ====================================================================

        # 1. æ„å»ºåˆå§‹å…ˆéªŒ
        Q_temp, mu_temp = build_prior(geom, cfg.prior)

        # 2. è®¡ç®—ä¸´æ—¶é˜ˆå€¼ï¼ˆç”¨äºDDIæ§åˆ¶ï¼Œä¸é”å®šï¼‰
        if hasattr(cfg.decision, 'tau_quantile') and cfg.decision.tau_quantile is not None:
            tau_temp = float(np.quantile(mu_temp, cfg.decision.tau_quantile))
            if not args.quiet:
                print(f"  ğŸ“Š Initial prior for DDI control:")
                print(f"     mean={mu_temp.mean():.3f}, std={mu_temp.std():.3f}")
                print(f"     Using tau_quantile={cfg.decision.tau_quantile:.2f} "
                      f"â†’ Ï„_temp={tau_temp:.3f}")
        else:
            # å›é€€åˆ°æˆæœ¬æ˜ å°„
            p_T = cfg.decision.prob_threshold
            tau_temp = float(np.quantile(mu_temp, p_T))
            if not args.quiet:
                print(f"  ğŸ“Š Using cost-based threshold: p_T={p_T:.3f} "
                      f"â†’ Ï„_temp={tau_temp:.3f}")

        # 3. åº”ç”¨DDIæ§åˆ¶
        if not args.quiet:
            print(f"  ğŸ¯ Applying DDI control (target={cfg.decision.target_ddi:.1%})...")

        try:
            Q_pr, mu_pr = build_prior_with_ddi(
                geom, cfg.prior, tau=tau_temp, target_ddi=cfg.decision.target_ddi
            )

            if not args.quiet:
                print(f"  âœ“ DDI control applied")
                print(f"     Final prior: mean={mu_pr.mean():.3f}, std={mu_pr.std():.3f}")

        except Exception as e:
            if not args.quiet:
                print(f"  âš ï¸  DDI control failed: {e}")
                print(f"  Falling back to standard prior without DDI control")
            Q_pr, mu_pr = Q_temp, mu_temp
            use_ddi = False  # æ ‡è®°DDIæ§åˆ¶å¤±è´¥

        # 4. ğŸ”¥ å…³é”®ï¼šåŸºäºDDIæ§åˆ¶åçš„æœ€ç»ˆå…ˆéªŒé”å®šé˜ˆå€¼
        if not args.quiet:
            print(f"  ğŸ”’ Locking threshold based on DDI-adjusted prior...")
        cfg.lock_decision_threshold(mu_pr, verbose=not args.quiet)

    else:
        # ====================================================================
        # ğŸ”¥ æƒ…å†µBï¼šä¸éœ€è¦DDIæ§åˆ¶
        # é¡ºåºï¼šæ„å»ºå…ˆéªŒ â†’ é”å®šé˜ˆå€¼
        # ====================================================================

        Q_pr, mu_pr = build_prior(geom, cfg.prior)
        cfg.lock_decision_threshold(mu_pr, verbose=not args.quiet)

    # ========================================================================
    # å¥åº·æ£€æŸ¥å’Œè¯Šæ–­
    # ========================================================================

    tau = cfg.decision.tau_iri

    # è®¡ç®—å…ˆéªŒç»Ÿè®¡ä¿¡æ¯
    mu_stats = {
        'min': mu_pr.min(),
        'max': mu_pr.max(),
        'mean': mu_pr.mean(),
        'median': np.median(mu_pr),
        'std': mu_pr.std(),
        'q10': np.quantile(mu_pr, 0.1),
        'q50': np.quantile(mu_pr, 0.5),
        'q90': np.quantile(mu_pr, 0.9),
    }

    # æ£€æŸ¥1ï¼šé˜ˆå€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´
    threshold_issues = []

    if tau < 0:
        threshold_issues.append(f"Threshold is negative (Ï„={tau:.3f})")
    elif tau > 5:
        threshold_issues.append(f"Threshold exceeds typical IRI range (Ï„={tau:.3f} > 5)")

    # æ£€æŸ¥2ï¼šé˜ˆå€¼æ˜¯å¦ä¸å…ˆéªŒåˆ†å¸ƒåŒ¹é…
    if tau < mu_stats['q10']:
        threshold_issues.append(f"Threshold below 10th percentile ({tau:.3f} < {mu_stats['q10']:.3f})")
    elif tau > mu_stats['max']:
        threshold_issues.append(f"Threshold exceeds maximum value ({tau:.3f} > {mu_stats['max']:.3f})")

    # æ£€æŸ¥3ï¼šå…ˆéªŒåˆ†å¸ƒæ˜¯å¦åˆç†
    if mu_stats['mean'] < -2 or mu_stats['mean'] > 5:
        threshold_issues.append(f"Prior mean unusual ({mu_stats['mean']:.3f})")

    if mu_stats['median'] < -1 or mu_stats['median'] > 4:
        threshold_issues.append(f"Prior median unusual ({mu_stats['median']:.3f})")

    # å¦‚æœæœ‰é—®é¢˜ï¼Œæ˜¾ç¤ºè¯¦ç»†è¯Šæ–­
    if threshold_issues:
        print(f"\n  âš ï¸  THRESHOLD DIAGNOSTICS")
        print(f"  {'=' * 68}")
        print(f"  Locked threshold: Ï„ = {tau:.3f}")
        print(f"\n  Issues detected:")
        for issue in threshold_issues:
            print(f"    â€¢ {issue}")

        print(f"\n  ğŸ“Š Prior distribution:")
        print(f"    Range: [{mu_stats['min']:.3f}, {mu_stats['max']:.3f}]")
        print(f"    Mean: {mu_stats['mean']:.3f}, Median: {mu_stats['median']:.3f}, Std: {mu_stats['std']:.3f}")
        print(f"    Quantiles: p10={mu_stats['q10']:.3f}, p50={mu_stats['q50']:.3f}, p90={mu_stats['q90']:.3f}")

        if use_ddi:
            print(f"\n  â„¹ï¸  DDI control was applied (target={cfg.decision.target_ddi:.1%})")
            print(f"  Recommendations:")
            print(f"    1. Lower target_ddi (try 0.10-0.20 instead of {cfg.decision.target_ddi:.2f})")
            print(f"    2. Disable DDI control (set target_ddi: null)")
            print(f"    3. Adjust tau_quantile (try 0.75-0.80 instead of current value)")
            print(f"    4. Modify prior.mu_prior_mean to center distribution better")
        else:
            print(f"\n  â„¹ï¸  No DDI control")
            print(f"  Recommendations:")
            print(f"    1. Check prior.mu_prior_mean in config (affects distribution center)")
            print(f"    2. Adjust tau_quantile (try lower values like 0.75)")
            print(f"    3. Verify prior variance settings")

        print(f"  {'=' * 68}")

        # ä¸¥é‡é—®é¢˜æ—¶å¯ä»¥é€‰æ‹©ç»ˆæ­¢
        if tau < -5 or tau > 10:
            print(f"\n  âŒ CRITICAL: Threshold extremely unusual, aborting experiment")
            print(f"  Please fix configuration before proceeding")
            sys.exit(1)

    if not args.quiet:
        print(f"âœ… Prior setup complete (Ï„={tau:.3f})")

    # ========================================================================
    # è®¡ç®—å®Œæ•´çš„å…ˆéªŒæ ‡å‡†å·®ï¼ˆç”¨äºåç»­è¯„ä¼°ï¼‰
    # ========================================================================

    from inference import SparseFactor, compute_posterior_variance_diagonal

    factor_pr = SparseFactor(Q_pr)
    var_pr = compute_posterior_variance_diagonal(factor_pr, indices=None)
    sigma_pr = np.sqrt(np.maximum(var_pr, 1e-12))

    # å¯é€‰ï¼šè·å–åŸŸç¼©æ”¾å› å­
    if hasattr(cfg, 'economics') and cfg.economics is not None:
        scale_factor = cfg.get_domain_scale_factor(verbose=not args.quiet)
    else:
        scale_factor = 1.0

    # ========================================================================
    # ç”ŸæˆçœŸå®çŠ¶æ€å’Œä¼ æ„Ÿå™¨
    # ========================================================================

    x_true = sample_gmrf(Q_pr, mu_pr, rng)
    np.save(output_dir / 'x_true.npy', x_true)

    sensors = generate_sensor_pool(geom, cfg.sensors, rng)

    if not args.quiet:
        print(f"  Generated {len(sensors)} heterogeneous sensors:")

        # ä¼ æ„Ÿå™¨ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for s in sensors:
            type_counts[s.type_name] = type_counts.get(s.type_name, 0) + 1

        print(f"    Type distribution:")
        for stype, count in sorted(type_counts.items()):
            print(f"      {stype}: {count} ({count / len(sensors) * 100:.1f}%)")

        costs = [s.cost for s in sensors]
        noises = [s.noise_var ** 0.5 for s in sensors]
        print(f"    Cost range: Â£{min(costs):.0f} - Â£{max(costs):.0f}")
        print(f"    Noise std range: {min(noises):.3f} - {max(noises):.3f}")

    # å…¨å±€æµ‹è¯•é›†
    n_test = min(200, geom.n)
    test_idx_global = rng.choice(geom.n, size=n_test, replace=False)

    # ========================================================================
    # è¿è¡Œæ–¹æ³•è¯„ä¼°
    # ========================================================================

    if not args.quiet:
        print(f"\nğŸš€ Running methods: {', '.join(cfg.selection.methods)}")

    all_results = {}
    methods = get_available_methods(cfg)

    for method_name in methods:
        method_start = datetime.now()
        try:
            results = run_method_evaluation(
                method_name=method_name,
                cfg=cfg,
                geom=geom,
                Q_pr=Q_pr,
                mu_pr=mu_pr,
                x_true=x_true,
                sensors=sensors,
                test_idx_global=test_idx_global,
                use_parallel=args.parallel,
                n_workers=args.workers,
                verbose=not args.quiet
            )
            all_results[method_name] = results

            method_elapsed = (datetime.now() - method_start).total_seconds()
            if not args.quiet:
                print(f"âœ… {method_name} completed in {method_elapsed:.1f}s")
        except Exception as e:
            if not args.quiet:
                print(f"âŒ {method_name} failed: {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    # ========================================================================
    # ä¿å­˜ç»“æœ
    # ========================================================================

    import pickle
    with open(output_dir / 'results_raw.pkl', 'wb') as f:
        pickle.dump(all_results, f)

    # è½¬æ¢ä¸ºDataFrame
    try:
        df_results = aggregate_results_for_visualization(all_results)
        if not df_results.empty:
            df_results.to_csv(output_dir / 'results_aggregated.csv', index=False)
            if not args.quiet:
                print(f"ğŸ’¾ Saved {len(df_results)} result rows")
    except Exception as e:
        if not args.quiet:
            print(f"âš ï¸ DataFrame conversion failed: {e}")
        df_results = pd.DataFrame()

    # ========================================================================
    # å¯è§†åŒ–
    # ========================================================================

    if not args.skip_viz and not df_results.empty:
        if not args.quiet:
            print(f"\nğŸ“Š Generating visualizations...")
        try:
            scenario = detect_scenario_from_config(cfg)
            generate_all_visualizations_v2(
                all_results=all_results,
                df_results=df_results,
                geom=geom,
                sensors=sensors,
                Q_pr=Q_pr,
                mu_pr=mu_pr,
                output_dir=output_dir,
                config=cfg,
                scenario=scenario
            )
            if not args.quiet:
                print(f"âœ… Visualization complete")
        except Exception as e:
            if not args.quiet:
                print(f"âŒ Visualization failed: {str(e)}")

    # ========================================================================
    # å®éªŒæ€»ç»“
    # ========================================================================

    total_elapsed = (datetime.now() - t_start).total_seconds()
    if not args.quiet:
        print(f"\n{exp_prefix}âœ… Experiment completed in {total_elapsed:.1f}s")
        print(f"ğŸ“ Results saved to: {output_dir}")

    return {
        'config': cfg,
        'output_dir': output_dir,
        'results': all_results,
        'elapsed_time': total_elapsed,
        'success': len(all_results) > 0,
        'threshold': tau,
        'prior_stats': mu_stats,
        'domain_scale_factor': scale_factor
    }

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """
    ğŸ”¥ å¢å¼ºçš„ä¸»å‡½æ•° - æ”¯æŒå‚æ•°æ‰«æå’Œçµæ´»é…ç½®
    """
    args = parse_arguments()

    verbose = not args.quiet

    if verbose:
        print("=" * 70)
        print("  RDT-VoI å‚æ•°åŒ–å®éªŒæ¡†æ¶")
        print("=" * 70)

    # 1. åŠ è½½åŸºç¡€é…ç½®
    try:
        base_cfg = load_config(args.config)
        if verbose:
            print(f"âœ… Loaded config: {args.config}")
    except Exception as e:
        print(f"âŒ Failed to load config: {e}")
        sys.exit(1)

    # 2. åº”ç”¨é¢„è®¾
    if args.preset:
        try:
            base_cfg = base_cfg.apply_preset(args.preset, verbose=verbose)
            if verbose:
                print(f"âœ… Applied preset: {args.preset}")
        except Exception as e:
            print(f"âŒ Failed to apply preset {args.preset}: {e}")
            sys.exit(1)

    # 3. åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–
    base_cfg = apply_cli_overrides(base_cfg, args)

    # 4. åˆ›å»ºå®éªŒé…ç½®åˆ—è¡¨
    try:
        configs = create_experiment_configs(base_cfg, args)
        if verbose:
            print(f"ğŸ“‹ Created {len(configs)} experiment configuration(s)")
    except Exception as e:
        print(f"âŒ Failed to create experiment configs: {e}")
        sys.exit(1)

    # 5. Dry runæ¨¡å¼
    if args.dry_run:
        print(f"\nğŸ” DRY RUN - Parameter combinations:")
        for i, cfg in enumerate(configs):
            print(f"\n  Experiment {i+1}: {cfg.experiment.name}")
            print(f"    DDI: {getattr(cfg.decision, 'target_ddi', 'N/A')}")
            print(f"    L_FN: Â£{cfg.decision.L_FN_gbp:,.0f}")
            print(f"    L_FP: Â£{cfg.decision.L_FP_gbp:,.0f}")
            print(f"    Budgets: {cfg.selection.budgets}")
            print(f"    Methods: {cfg.selection.methods}")
        print(f"\nâœ… Dry run complete. Use without --dry-run to execute.")
        return

    # 6. æ‰§è¡Œå®éªŒ
    successful_experiments = []
    failed_experiments = []

    total_start = datetime.now()

    for i, cfg in enumerate(configs):
        try:
            result = run_single_experiment(cfg, args, exp_index=i, total_experiments=len(configs))
            if result['success']:
                successful_experiments.append(result)
            else:
                failed_experiments.append(result)
        except Exception as e:
            if verbose:
                print(f"âŒ Experiment {i+1} failed: {str(e)}")
            failed_experiments.append({
                'config': cfg,
                'error': str(e),
                'success': False
            })
            import traceback
            traceback.print_exc()

    # 7. æ€»ç»“æŠ¥å‘Š
    total_elapsed = (datetime.now() - total_start).total_seconds()

    if verbose:
        print(f"\n" + "=" * 70)
        print(f"  EXPERIMENT SUMMARY")
        print(f"=" * 70)
        print(f"âœ… Successful: {len(successful_experiments)}")
        print(f"âŒ Failed: {len(failed_experiments)}")
        print(f"â±ï¸  Total time: {total_elapsed:.1f}s")

        if successful_experiments:
            print(f"\nğŸ“ Output directories:")
            for result in successful_experiments:
                print(f"  - {result['output_dir']}")

    # é€€å‡ºç 
    if failed_experiments and not successful_experiments:
        sys.exit(1)
    elif failed_experiments:
        sys.exit(2)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()