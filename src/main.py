"""
Main experimental script for RDT-VoI simulation.
Runs complete benchmarking suite with spatial CV.
"""

from pathlib import Path
from datetime import datetime
import json
import pickle
import sys
import warnings
import numpy as np
import pandas as pd
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from visualization_helpers import generate_expert_plots

from matplotlib import pyplot as plt

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))
from typing import Dict, List
import pickle
import time
from config import load_config
from geometry import build_grid2d_geometry
from spatial_field import build_prior, sample_gmrf
from sensors import generate_sensor_pool
from inference import compute_posterior, compute_posterior_variance_diagonal
from sensors import get_observation

from selection import greedy_mi, greedy_aopt, uniform_selection, random_selection
from evaluation import spatial_block_cv, compute_metrics, morans_i
# ğŸ”¥ Import new method wrappers
from method_wrappers import (
    get_selection_method,
    get_available_methods,
    should_use_evi
)

# ğŸ”¥ Import visualization
from visualization import (
    plot_budget_curves,
    plot_performance_profile,
    plot_critical_difference,
    plot_marginal_mi,
    plot_mi_evi_correlation,
    plot_marginal_efficiency,
    plot_calibration_diagnostics,
    plot_sensor_placement_map, setup_style
)

# ğŸ”¥ æ·»åŠ è‡ªå®šä¹‰JSON encoder
class NumpyEncoder(json.JSONEncoder):
    """å¤„ç†numpyç±»å‹çš„JSON encoder"""
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# âœ… åœ¨é¡¶å±‚å®šä¹‰æ–¹æ³•æ˜ å°„å‡½æ•°ï¼ˆå¯ä»¥è¢« pickleï¼‰

def run_single_fold_worker(fold_data: Dict) -> Dict:
    """
    å®Œæ•´å¢å¼ºç‰ˆ CV fold workerï¼Œæ”¯æŒæ‰€æœ‰æ–°æŒ‡æ ‡

    åŒ…å«ï¼š
    - å…ˆéªŒ/åéªŒæŸå¤±å¯¹æ¯”
    - ROI è®¡ç®—
    - DDI ç»Ÿè®¡
    - è¡ŒåŠ¨å—é™è¯„ä¼°
    - å®Œæ•´è¯Šæ–­ä¿¡æ¯

    Args:
        fold_data: Dictionary containing:
            - train_idx, test_idx: CV split
            - selection_method: Function to call
            - k: Budget
            - Q_pr, mu_pr: Prior
            - x_true: True state
            - sensors: Sensor pool
            - decision_config: Decision model
            - geom: Geometry object
            - rng_seed: Random seed for this fold

    Returns:
        Dictionary with comprehensive metrics and selection results
    """
    import time
    import warnings
    import numpy as np
    from inference import compute_posterior, compute_posterior_variance_diagonal, SparseFactor
    from sensors import get_observation
    from evaluation import compute_metrics, morans_i
    from decision import expected_loss

    # =========================================================================
    # è§£åŒ…æ•°æ®
    # =========================================================================
    train_idx = fold_data['train_idx']
    test_idx = fold_data['test_idx']
    selection_method = fold_data['selection_method']
    k = fold_data['k']
    Q_pr = fold_data['Q_pr']
    mu_pr = fold_data['mu_pr']
    x_true = fold_data['x_true']
    sensors = fold_data['sensors']
    decision_config = fold_data['decision_config']
    geom = fold_data['geom']
    rng = np.random.default_rng(fold_data['rng_seed'])

    try:
        # =====================================================================
        # 1. è®¡ç®—å…ˆéªŒæŸå¤±ï¼ˆç”¨äº ROIï¼‰
        # =====================================================================
        t_prior_start = time.time()

        # è·å–å†³ç­–é˜ˆå€¼
        tau = decision_config.get_threshold(mu_pr)

        # è®¡ç®—æµ‹è¯•é›†ä¸Šçš„å…ˆéªŒæ–¹å·®
        factor_pr = SparseFactor(Q_pr)
        var_pr_test = compute_posterior_variance_diagonal(factor_pr, test_idx)
        sigma_pr_test = np.sqrt(np.maximum(var_pr_test, 1e-12))

        # å…ˆéªŒå†³ç­–æŸå¤±
        prior_loss = expected_loss(
            mu_pr[test_idx],
            sigma_pr_test,
            decision_config,
            test_indices=np.arange(len(test_idx)),
            tau=tau
        )

        prior_time = time.time() - t_prior_start

        # =====================================================================
        # 2. ä¼ æ„Ÿå™¨é€‰æ‹©
        # =====================================================================
        t_sel_start = time.time()
        selection_result = selection_method(sensors, k, Q_pr, mu_pr)
        selection_time = time.time() - t_sel_start

        selected_sensors = [sensors[i] for i in selection_result.selected_ids]
        sensor_cost = selection_result.total_cost

        # =====================================================================
        # 3. ç”Ÿæˆè§‚æµ‹
        # =====================================================================
        y, H, R = get_observation(x_true, selected_sensors, rng)

        # =====================================================================
        # 4. è®¡ç®—åéªŒ
        # =====================================================================
        t_inf_start = time.time()
        mu_post, factor_post = compute_posterior(Q_pr, mu_pr, H, R, y)
        inference_time = time.time() - t_inf_start

        # æµ‹è¯•é›†åéªŒä¸ç¡®å®šæ€§
        var_post_test = compute_posterior_variance_diagonal(factor_post, test_idx)
        sigma_post_test = np.sqrt(np.maximum(var_post_test, 1e-12))

        # æ‰©å±•åˆ°å…¨åŸŸï¼ˆä¾› compute_metrics ä½¿ç”¨ï¼‰
        sigma_post = np.zeros(len(mu_post))
        sigma_post[test_idx] = sigma_post_test

        # =====================================================================
        # 5. åŸºç¡€æŒ‡æ ‡ï¼ˆè°ƒç”¨å·²æ›´æ–°çš„ compute_metricsï¼‰
        # =====================================================================
        # å‡è®¾ compute_metrics å·²è¿”å›ï¼šRMSE, MAE, RÂ², loss, coverage, MSSE, z_scores
        metrics = compute_metrics(
            mu_post, sigma_post, x_true, test_idx, decision_config
        )

        posterior_loss = metrics['expected_loss_gbp']

        # =====================================================================
        # 6. ROI å’Œæˆæœ¬æ•ˆç‡
        # =====================================================================
        savings = prior_loss - posterior_loss

        if sensor_cost > 0:
            roi = (savings - sensor_cost) / sensor_cost
            cost_efficiency = savings / sensor_cost
        else:
            roi = np.inf if savings > 0 else 0.0
            cost_efficiency = np.inf if savings > 0 else 0.0

        metrics['roi'] = float(roi)
        metrics['cost_efficiency'] = float(cost_efficiency)
        metrics['prior_loss_gbp'] = float(prior_loss)
        metrics['posterior_loss_gbp'] = float(posterior_loss)
        metrics['savings_gbp'] = float(savings)

        # =====================================================================
        # 7. DDIï¼ˆå†³ç­–éš¾åº¦æŒ‡æ•°ï¼‰ç»Ÿè®¡
        # =====================================================================
        try:
            from spatial_field import compute_ddi

            # æµ‹è¯•é›† DDI
            ddi_test = compute_ddi(
                mu_post[test_idx],
                sigma_post_test,
                tau,
                k=1.0
            )
            metrics['ddi_test'] = float(ddi_test)

            # å…ˆéªŒ DDIï¼ˆé‡‡æ ·ä¼°è®¡ï¼‰
            sample_size = min(200, len(mu_pr))
            sample_idx = rng.choice(len(mu_pr), size=sample_size, replace=False)
            var_pr_sample = compute_posterior_variance_diagonal(factor_pr, sample_idx)
            sigma_pr_sample = np.sqrt(np.maximum(var_pr_sample, 1e-12))

            ddi_prior = compute_ddi(
                mu_pr[sample_idx],
                sigma_pr_sample,
                tau,
                k=1.0
            )
            metrics['ddi_prior'] = float(ddi_prior)

        except Exception as e:
            warnings.warn(f"DDI computation failed: {e}")
            metrics['ddi_test'] = np.nan
            metrics['ddi_prior'] = np.nan

        # =====================================================================
        # 8. è¡ŒåŠ¨å—é™è¯„ä¼°ï¼ˆå¦‚æœé…ç½®äº† K_actionï¼‰
        # =====================================================================
        if hasattr(decision_config, 'K_action') and decision_config.K_action is not None:
            try:
                K_action = decision_config.K_action

                from scipy.stats import norm
                from decision import conditional_risk

                # è®¡ç®—åéªŒæ•…éšœæ¦‚ç‡
                p_failure = 1.0 - norm.cdf(
                    (tau - mu_post[test_idx]) / np.maximum(sigma_post_test, 1e-12)
                )

                # é€‰æ‹©é£é™©æœ€é«˜çš„ K ä¸ªä½ç½®ï¼ˆåœ¨æµ‹è¯•é›†å†…ï¼‰
                if K_action < len(test_idx):
                    top_k_local = np.argsort(p_failure)[-K_action:]
                else:
                    top_k_local = np.arange(len(test_idx))

                # è®¡ç®—è¡ŒåŠ¨å—é™åçš„æŸå¤±
                constrained_risks = np.zeros(len(test_idx))

                for i in range(len(test_idx)):
                    global_idx = test_idx[i]

                    if i in top_k_local:
                        # ç»´æŠ¤ï¼šæ‰¿æ‹… L_TP æˆ– L_FP
                        if x_true[global_idx] > tau:
                            constrained_risks[i] = decision_config.L_TP_gbp
                        else:
                            constrained_risks[i] = decision_config.L_FP_gbp
                    else:
                        # ä¸ç»´æŠ¤ï¼šæ‰¿æ‹… L_FN æˆ– L_TN
                        if x_true[global_idx] > tau:
                            constrained_risks[i] = decision_config.L_FN_gbp
                        else:
                            constrained_risks[i] = decision_config.L_TN_gbp

                constrained_loss = constrained_risks.mean()

                # å‘½ä¸­ç‡ï¼šçœŸå®è¶…é˜ˆå€¼çš„ç‚¹ä¸­ï¼Œæˆ‘ä»¬ç»´æŠ¤äº†å¤šå°‘
                true_exceed = x_true[test_idx] > tau
                if true_exceed.sum() > 0:
                    hit_rate = np.sum(np.isin(top_k_local, np.where(true_exceed)[0])) / true_exceed.sum()
                else:
                    hit_rate = 1.0

                # æ·»åŠ åˆ°æŒ‡æ ‡
                metrics['action_K'] = int(K_action)
                metrics['action_constrained_loss'] = float(constrained_loss)
                metrics['action_regret'] = float(constrained_loss - posterior_loss)
                metrics['action_hit_rate'] = float(hit_rate)
                metrics['action_n_true_exceed'] = int(true_exceed.sum())
                metrics['action_n_maintained'] = int(len(top_k_local))

            except Exception as e:
                warnings.warn(f"Action-constrained evaluation failed: {e}")

        # =====================================================================
        # 9. ç©ºé—´è¯Šæ–­ï¼ˆMoran's Iï¼‰
        # =====================================================================
        residuals = mu_post - x_true

        if geom.adjacency is not None:
            try:
                I_stat, I_pval = morans_i(
                    residuals[test_idx],
                    geom.adjacency[test_idx][:, test_idx],
                    n_permutations=999,
                    rng=rng
                )
                metrics['morans_i'] = float(I_stat)
                metrics['morans_pval'] = float(I_pval)
            except Exception as e:
                warnings.warn(f"Moran's I computation failed: {e}")
                metrics['morans_i'] = np.nan
                metrics['morans_pval'] = np.nan

        # =====================================================================
        # 10. æ—¶é—´ç»Ÿè®¡
        # =====================================================================
        metrics['prior_computation_time_sec'] = float(prior_time)
        metrics['selection_time_sec'] = float(selection_time)
        metrics['inference_time_sec'] = float(inference_time)
        metrics['total_time_sec'] = float(prior_time + selection_time + inference_time)

        # =====================================================================
        # 11. ä¼ æ„Ÿå™¨é€‰æ‹©è¯Šæ–­
        # =====================================================================
        metrics['n_selected'] = len(selection_result.selected_ids)
        metrics['total_cost'] = float(sensor_cost)

        # ç±»å‹åˆ†å¸ƒ
        type_counts = {}
        for sid in selection_result.selected_ids:
            stype = sensors[sid].type_name
            type_counts[stype] = type_counts.get(stype, 0) + 1

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        metrics['type_counts'] = {k: int(v) for k, v in type_counts.items()}

        # æˆæœ¬åˆ†å¸ƒ
        selected_costs = [sensors[i].cost for i in selection_result.selected_ids]
        metrics['cost_mean'] = float(np.mean(selected_costs))
        metrics['cost_std'] = float(np.std(selected_costs))
        metrics['cost_min'] = float(np.min(selected_costs))
        metrics['cost_max'] = float(np.max(selected_costs))

        # å™ªå£°åˆ†å¸ƒ
        selected_noise_vars = [sensors[i].noise_var for i in selection_result.selected_ids]
        metrics['noise_mean'] = float(np.mean(selected_noise_vars))
        metrics['noise_std'] = float(np.std(selected_noise_vars))

        # =====================================================================
        # 12. è¿‘é˜ˆå€¼åŒºåŸŸç»Ÿè®¡ï¼ˆé¢å¤–è¯Šæ–­ï¼‰
        # =====================================================================
        # ç»Ÿè®¡é€‰æ‹©çš„ä¼ æ„Ÿå™¨ä¸­æœ‰å¤šå°‘åœ¨"å†³ç­–éš¾åº¦åŒº"
        try:
            selected_locs = [sensors[i].idxs[0] for i in selection_result.selected_ids]
            selected_gaps = np.abs(mu_pr[selected_locs] - tau)

            # ä¼°è®¡è¿™äº›ä½ç½®çš„å…ˆéªŒæ ‡å‡†å·®
            var_selected = compute_posterior_variance_diagonal(factor_pr, np.array(selected_locs))
            sigma_selected = np.sqrt(np.maximum(var_selected, 1e-12))

            # æœ‰å¤šå°‘ä¼ æ„Ÿå™¨åœ¨ 1Ïƒ é˜ˆå€¼å¸¦å†…
            near_threshold = selected_gaps <= sigma_selected
            metrics['frac_sensors_near_threshold'] = float(near_threshold.mean())

        except Exception as e:
            warnings.warn(f"Near-threshold statistics failed: {e}")
            metrics['frac_sensors_near_threshold'] = np.nan

        # =====================================================================
        # 13. è¿”å›å®Œæ•´ç»“æœ
        # =====================================================================
        return {
            'success': True,
            'metrics': metrics,
            'selection_result': selection_result,
            'mu_post': mu_post,
            'sigma_post': sigma_post,
            'residuals': residuals[test_idx],
            'test_idx': test_idx,
            'tau': tau,
            'prior_loss': prior_loss,
            'posterior_loss': posterior_loss,
            'savings': savings,
            'roi': roi
        }

    except Exception as e:
        import traceback
        error_msg = str(e)
        error_trace = traceback.format_exc()

        warnings.warn(f"Fold evaluation failed: {error_msg}")

        return {
            'success': False,
            'error': error_msg,
            'traceback': error_trace,
            'metrics': {},
            'selection_result': None,
            'mu_post': None,
            'sigma_post': None,
            'residuals': None,
            'test_idx': test_idx,
            'tau': None,
            'prior_loss': None,
            'posterior_loss': None,
            'savings': None,
            'roi': None
        }

def run_method_evaluation(method_name: str, cfg, geom, Q_pr, mu_pr,
                          x_true, sensors, test_idx_global=None) -> Dict:
    """
    ğŸ”¥ ä¿®å¤ç‰ˆï¼šè¿è¡Œå®Œæ•´çš„æ–¹æ³•è¯„ä¼°

    Run complete evaluation for one method across all budgets and CV folds.
    """
    print(f"\n{'=' * 70}")
    print(f"  Method: {method_name.upper()}")
    print(f"{'=' * 70}")

    rng = cfg.get_rng()

    # åˆ›å»ºé€‰æ‹©æ–¹æ³•wrapper
    try:
        selection_method = get_selection_method(
            method_name=method_name,
            config=cfg,
            geom=geom,
            x_true=x_true,
            test_idx=test_idx_global
        )
    except Exception as e:
        print(f"  âœ— Failed to create method wrapper: {e}")
        raise

    # ç”ŸæˆCV folds
    buffer_width = cfg.cv.buffer_width_multiplier * cfg.prior.correlation_length
    folds = spatial_block_cv(
        geom.coords,
        cfg.cv.k_folds,
        buffer_width,
        cfg.cv.block_strategy,
        rng
    )

    # æ‰“å°foldä¿¡æ¯
    for fold_idx, (train_idx, test_idx) in enumerate(folds):
        print(f"  Fold {fold_idx + 1}: train={len(train_idx)}, test={len(test_idx)}")

    results = {
        'budgets': {},
        'method_name': method_name,
        'n_folds': len(folds)
    }

    # éå†budgets
    for k in cfg.selection.budgets:
        print(f"\n  Budget k={k}")
        print(f"  {'-' * 50}")

        budget_results = {
            'fold_results': [],
            'fold_metrics': []
        }

        # éå†folds
        for fold_idx, (train_idx, test_idx) in enumerate(folds):
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåªå¯¹ EVI æ–¹æ³•æ£€æŸ¥æ˜¯å¦è·³è¿‡
            is_evi_method = method_name.lower() in ['greedy_evi', 'evi', 'greedy-evi', 'myopic_evi']

            if is_evi_method:
                # EVI æ–¹æ³•å¯èƒ½éœ€è¦è·³è¿‡æŸäº›é…ç½®ä»¥èŠ‚çœæ—¶é—´
                if not should_use_evi(method_name, k, fold_idx, cfg):
                    print(f"    Fold {fold_idx + 1}/{len(folds)}: SKIPPED (EVI subset)")
                    budget_results['fold_results'].append({
                        'success': False,
                        'skipped': True,
                        'reason': 'EVI budget/fold subset'
                    })
                    continue

            print(f"    Fold {fold_idx + 1}/{len(folds)}: train={len(train_idx)}, test={len(test_idx)}")

            # å‡†å¤‡foldæ•°æ®
            fold_data = {
                'train_idx': train_idx,
                'test_idx': test_idx,
                'selection_method': selection_method,
                'k': k,
                'Q_pr': Q_pr,
                'mu_pr': mu_pr,
                'x_true': x_true,
                'sensors': sensors,
                'decision_config': cfg.decision,
                'geom': geom,
                'rng_seed': rng.integers(0, 2 ** 31)
            }

            # è¿è¡Œfold
            try:
                fold_result = run_single_fold_worker(fold_data)
                budget_results['fold_results'].append(fold_result)

                if fold_result['success']:
                    metrics = fold_result['metrics']
                    budget_results['fold_metrics'].append(metrics)

                    # æ‰“å°å…³é”®æŒ‡æ ‡
                    print(f"        RMSE={metrics['rmse']:.3f}, "
                          f"Loss=Â£{metrics['expected_loss_gbp']:.0f}, "
                          f"Coverage={metrics['coverage_90'] * 100:.2f}%")
                else:
                    print(f"        âœ— FAILED: {fold_result.get('error', 'unknown error')}")

            except Exception as e:
                print(f"        âœ— Exception: {str(e)}")
                import traceback
                traceback.print_exc()

                budget_results['fold_results'].append({
                    'success': False,
                    'error': str(e),
                    'traceback': traceback.format_exc()
                })

        # è®¡ç®—budgetçº§åˆ«çš„ç»Ÿè®¡
        if budget_results['fold_metrics']:
            n_folds = len(budget_results['fold_metrics'])

            # èšåˆæŒ‡æ ‡
            aggregated = {}
            for key in budget_results['fold_metrics'][0].keys():
                # è·³è¿‡éæ•°å€¼æŒ‡æ ‡
                if key in ['z_scores', 'n_test', 'n_selected']:
                    continue

                values = [m[key] for m in budget_results['fold_metrics'] if key in m]
                if values and all(isinstance(v, (int, float)) for v in values):
                    aggregated[key] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'values': values
                    }

            budget_results['aggregated'] = aggregated

            print(f"\n    Summary (n={n_folds} folds):")
            for metric in ['expected_loss_gbp', 'rmse']:
                if metric in aggregated:
                    stats = aggregated[metric]

                    # ğŸ”¥ ä¿®å¤ï¼šæ¡ä»¶æ ¼å¼åŒ–
                    if 'loss' in metric:
                        mean_str = f"{stats['mean']:.0f}"
                        std_str = f"{stats['std']:.0f}"
                    else:
                        mean_str = f"{stats['mean']:.3f}"
                        std_str = f"{stats['std']:.3f}"

                    print(f"      {metric.replace('_', ' ').title()}: "
                          f"{mean_str} Â± {std_str}")
        else:
            print(f"\n    âš ï¸  No successful folds for budget k={k}")

        results['budgets'][k] = budget_results

    return results


def aggregate_results_for_visualization(all_results: Dict) -> pd.DataFrame:
    """
    ğŸ”¥ ä¿®å¤ç‰ˆï¼šå°†ç»“æœè½¬æ¢ä¸ºDataFrameä¾›å¯è§†åŒ–ä½¿ç”¨

    è¿‡æ»¤æ‰éæ ‡é‡å­—æ®µï¼ˆå­—å…¸ã€åˆ—è¡¨ç­‰ï¼‰
    """
    rows = []

    print("    å¼€å§‹èšåˆç»“æœ...")

    for method_name, method_data in all_results.items():
        print(f"      å¤„ç†æ–¹æ³•: {method_name}")

        if not isinstance(method_data, dict):
            print(f"        âš ï¸  è·³è¿‡ï¼šæ•°æ®ç±»å‹é”™è¯¯ ({type(method_data)})")
            continue

        budgets_data = method_data.get('budgets', {})

        if not budgets_data:
            print(f"        âš ï¸  è·³è¿‡ï¼šæ— budgetsæ•°æ®")
            continue

        print(f"        æ‰¾åˆ° {len(budgets_data)} ä¸ªbudgets")

        for budget, budget_data in budgets_data.items():
            if not isinstance(budget_data, dict):
                continue

            fold_results = budget_data.get('fold_results', [])

            if not fold_results:
                continue

            valid_folds = 0

            for fold_idx, fold_res in enumerate(fold_results):
                if not isinstance(fold_res, dict):
                    continue

                if not fold_res.get('success', False):
                    continue

                metrics = fold_res.get('metrics', {})

                if not metrics or not isinstance(metrics, dict):
                    continue

                valid_folds += 1

                # ğŸ”¥ ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºè¡Œï¼Œä½†è·³è¿‡éæ ‡é‡å­—æ®µ
                for metric_name, metric_value in metrics.items():
                    # è·³è¿‡éæ ‡é‡å­—æ®µ
                    if metric_name in ['z_scores', 'type_counts']:
                        continue

                    # è·³è¿‡å†…éƒ¨å­—æ®µ
                    if metric_name.startswith('_'):
                        continue

                    # ğŸ”¥ ç¡®ä¿å€¼æ˜¯æ ‡é‡
                    if isinstance(metric_value, (list, np.ndarray, dict)):
                        continue

                    if metric_value is None:
                        continue

                    # è½¬æ¢ä¸ºæ ‡é‡
                    try:
                        scalar_value = float(metric_value)
                    except (ValueError, TypeError):
                        continue

                    if np.isnan(scalar_value):
                        continue

                    rows.append({
                        'method': method_name,
                        'budget': int(budget),
                        'fold': fold_idx + 1,
                        'metric': metric_name,
                        'value': scalar_value
                    })

            if valid_folds > 0:
                print(f"          Budget {budget}: {valid_folds} ä¸ªæœ‰æ•ˆfolds")

    if not rows:
        warnings.warn("æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœå¯ä»¥èšåˆ")
        return pd.DataFrame()

    # åˆ›å»ºDataFrame
    df = pd.DataFrame(rows)

    # ğŸ”¥ è®¡ç®—ç»Ÿè®¡é‡
    stats_rows = []
    for (method, budget, metric), group in df.groupby(['method', 'budget', 'metric']):
        values = group['value'].values
        stats_rows.append({
            'method': method,
            'budget': budget,
            'fold': None,  # èšåˆç»Ÿè®¡
            'metric': metric,
            'value': np.mean(values),
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
            'n_folds': len(values)
        })

    # åˆå¹¶åŸå§‹æ•°æ®å’Œç»Ÿè®¡æ•°æ®
    df_stats = pd.DataFrame(stats_rows)
    df_combined = pd.concat([df, df_stats], ignore_index=True)

    print(f"    âœ“ èšåˆå®Œæˆ: {len(df)} è¡ŒåŸå§‹æ•°æ® + {len(df_stats)} è¡Œç»Ÿè®¡æ•°æ®")
    print(f"    âœ“ æ–¹æ³•: {sorted(df['method'].unique())}")
    print(f"    âœ“ æŒ‡æ ‡: {len(df['metric'].unique())} ä¸ª")

    return df_combined


def serialize_sparse_matrix(mat):
    """å°†ç¨€ç–çŸ©é˜µåºåˆ—åŒ–ä¸ºå­—å…¸"""
    mat_csr = mat.tocsr()
    return {
        'data': mat_csr.data,
        'indices': mat_csr.indices,
        'indptr': mat_csr.indptr,
        'shape': mat_csr.shape
    }


def serialize_geometry(geom):
    """å°† Geometry å¯¹è±¡åºåˆ—åŒ–"""
    return {
        'mode': geom.mode,
        'n': geom.n,
        'coords': geom.coords,
        'adj_data': geom.adjacency.data,
        'adj_indices': geom.adjacency.indices,
        'adj_indptr': geom.adjacency.indptr,
        'lap_data': geom.laplacian.data,
        'lap_indices': geom.laplacian.indices,
        'lap_indptr': geom.laplacian.indptr,
        'h': geom.h
    }


def serialize_sensors(sensors):
    """å°†ä¼ æ„Ÿå™¨åˆ—è¡¨åºåˆ—åŒ–"""
    return [
        {
            'id': s.id,
            'idxs': s.idxs,
            'weights': s.weights,
            'noise_var': s.noise_var,
            'cost': s.cost,
            'type_name': s.type_name
        }
        for s in sensors
    ]


def run_cv_experiment_parallel(geom, Q_pr, mu_pr, x_true, sensors,
                               method_name, k, cv_config,
                               decision_config, seed, n_workers=5):
    """
    å¹¶è¡Œè¿è¡ŒCVå®éªŒ

    Args:
        geom: å‡ ä½•å¯¹è±¡
        Q_pr, mu_pr: å…ˆéªŒå‚æ•°
        x_true: çœŸå®çŠ¶æ€
        sensors: å€™é€‰ä¼ æ„Ÿå™¨æ± 
        method_name: æ–¹æ³•åç§°
        k: é¢„ç®—
        cv_config: CVé…ç½®
        decision_config: å†³ç­–é…ç½®
        seed: éšæœºç§å­
        n_workers: å¹¶è¡Œworkeræ•°é‡

    Returns:
        results: CVç»“æœå­—å…¸
    """
    from evaluation import spatial_block_cv
    import numpy as np
    from concurrent.futures import ProcessPoolExecutor, as_completed

    # ç”ŸæˆCVæŠ˜
    buffer_width = 15.0  # æˆ–ä» cv_config è·å–
    folds = spatial_block_cv(
        geom.coords,
        k_folds=cv_config.get('k_folds', 3),
        buffer_width=buffer_width,
        block_strategy=cv_config.get('block_strategy', 'kmeans'),
        rng=np.random.default_rng(seed)
    )

    print(f"\n  Running {len(folds)} folds in parallel with {n_workers} workers...")

    # æäº¤æ‰€æœ‰ä»»åŠ¡
    futures = []
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        for fold_idx, (train_idx, test_idx) in enumerate(folds):
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼ é€’æ‰€æœ‰å‚æ•°
            future = executor.submit(
                run_single_fold_worker,
                method_name,  # å‚æ•°1
                geom,  # å‚æ•°2
                Q_pr,  # å‚æ•°3
                mu_pr,  # å‚æ•°4
                x_true,  # å‚æ•°5
                sensors,  # å‚æ•°6
                k,  # å‚æ•°7
                train_idx,  # å‚æ•°8
                test_idx,  # å‚æ•°9
                decision_config,  # å‚æ•°10
                fold_idx,  # å‚æ•°11
                seed  # å‚æ•°12
            )
            futures.append(future)

        # æ”¶é›†ç»“æœ
        fold_results = []
        for future in as_completed(futures):
            try:
                fold_idx, metrics = future.result()
                fold_results.append(metrics)
            except Exception as e:
                print(f"\nâœ— ERROR: {e}")
                import traceback
                traceback.print_exc()
                raise

    # æŒ‰fold_idxæ’åº
    # fold_results.sort(key=lambda x: x.get('fold_idx', 0))

    # èšåˆç»“æœ
    aggregated = {}
    for key in fold_results[0].keys():
        # è·³è¿‡éæ•°å€¼å­—æ®µ
        if key in ['selection_result', 'mu_post', 'x_true', 'test_idx', 'fold_idx']:
            continue

        values = np.array([fr[key] for fr in fold_results])
        aggregated[key] = {
            'mean': float(values.mean()),
            'std': float(values.std()),
            'values': values.tolist()
        }

    return {
        'fold_results': fold_results,
        'aggregated': aggregated
    }

def create_output_dir(cfg) -> Path:
    """Create timestamped output directory."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(cfg.experiment.output_dir) / f"{cfg.experiment.name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)

    (output_dir / "curves").mkdir(exist_ok=True)
    (output_dir / "diagnostics").mkdir(exist_ok=True)

    # Save config
    cfg.save_to(output_dir)

    # Save environment
    import subprocess
    try:
        env_info = subprocess.check_output(['pip', 'freeze']).decode()
        with open(output_dir / "environment.txt", 'w', encoding='utf-8') as f:
            f.write(env_info)
    except:
        pass

    return output_dir


def run_milestone_m1(cfg, output_dir):
    """
    Milestone M1: Single-fold validation on small grid.
    Verify monotonic decrease and diminishing returns.
    """
    print("\n" + "="*60)
    print("MILESTONE M1: Small-scale validation")
    print("="*60)

    rng = cfg.get_rng()

    # Override config for M1
    nx = ny = int(np.sqrt(cfg.acceptance.m1_grid_size))
    geom = build_grid2d_geometry(nx, ny, h=cfg.geometry.h)

    print(f"Grid: {nx}Ã—{ny} = {geom.n} locations")

    # Build prior and sample true state
    Q_pr, mu_pr = build_prior(geom, cfg.prior)
    x_true = sample_gmrf(Q_pr, mu_pr, rng)

    # Generate sensors
    sensors = generate_sensor_pool(geom, cfg.sensors, rng)
    print(f"Sensor pool: {len(sensors)} candidates")

    # åªè¿è¡Œä¸€æ¬¡æœ€å¤§ budget
    max_budget = max(cfg.acceptance.m1_budgets)
    print(f"\nRunning Greedy-MI with budget k={max_budget}")
    result = greedy_mi(sensors, max_budget, Q_pr, lazy=True)

    # Save marginal VoI plot
    plot_marginal_mi(
        result,
        output_dir / "curves" / f"m1_marginal_voi_full.png"
    )

    # Check monotonicity
    is_monotonic = all(
        result.objective_values[i] <= result.objective_values[i+1]
        for i in range(len(result.objective_values)-1)
    )

    # Check diminishing returns
    is_diminishing = all(
        result.marginal_gains[i] >= result.marginal_gains[i+1] - 1e-6
        for i in range(len(result.marginal_gains)-1)
    )

    print("\n" + "-"*60)
    print(f"M1 RESULTS:")
    print(f"  Total steps: {len(result.objective_values)}")
    print(f"  Final MI: {result.objective_values[-1]:.3f} nats")
    print(f"  Monotonic increase: {'âœ“ PASS' if is_monotonic else 'âœ— FAIL'}")
    print(f"  Diminishing returns: {'âœ“ PASS' if is_diminishing else 'âœ— FAIL'}")
    print(f"\n  First 5 marginal gains: {result.marginal_gains[:5]}")
    print(f"  Last 5 marginal gains: {result.marginal_gains[-5:]}")
    print("-"*60)

    # Save results
    m1_summary = {
        'monotonic': is_monotonic,
        'diminishing': is_diminishing,
        'final_mi': float(result.objective_values[-1]),
        'marginal_gains': [float(x) for x in result.marginal_gains],
        'max_budget': max_budget
    }

    with open(output_dir / "m1_summary.json", 'w', encoding='utf-8') as f:
        json.dump(m1_summary, f, indent=2, cls=NumpyEncoder)

    return m1_summary


def run_milestone_m2(cfg, output_dir):
    """
    Milestone M2: Baseline comparison with spatial CV.
    """
    print("\n" + "="*60)
    print("MILESTONE M2: Baseline comparison")
    print("="*60)

    rng = cfg.get_rng()

    # Build full geometry
    geom = build_grid2d_geometry(cfg.geometry.nx, cfg.geometry.ny, h=cfg.geometry.h)
    print(f"Grid: {cfg.geometry.nx}Ã—{cfg.geometry.ny} = {geom.n} locations")

    # Build prior and sample
    Q_pr, mu_pr = build_prior(geom, cfg.prior)
    x_true = sample_gmrf(Q_pr, mu_pr, rng)

    # Generate sensors
    sensors = generate_sensor_pool(geom, cfg.sensors, rng)
    print(f"Sensor pool: {len(sensors)} candidates")

    # æ–¹æ³•åç§°åˆ—è¡¨
    method_names = ['Greedy-MI', 'Greedy-A', 'Uniform', 'Random']

    # Run experiments for each method and budget
    all_results = {}

    for method_name in method_names:
        print(f"\n{'='*60}")
        print(f"Method: {method_name}")
        print(f"{'='*60}")

        all_results[method_name] = {}

        for k in cfg.selection.budgets:
            print(f"\nBudget k={k}")

            # ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬
            cv_results = run_cv_experiment_parallel(
                geom=geom,
                Q_pr=Q_pr,
                mu_pr=mu_pr,
                x_true=x_true,
                sensors=sensors,
                method_name=method_name,
                k=k,
                cv_config=cfg.cv.__dict__,  # æˆ–ç›´æ¥ä¼ cfg.cv
                decision_config=cfg.decision,
                seed=cfg.experiment.seed,
                n_workers=5
            )

            all_results[method_name][k] = cv_results

    # Save results
    with open(output_dir / "m2_results.pkl", 'wb') as f:
        pickle.dump(all_results, f)

    # Generate plots
    print("\nGenerating plots...")

    # Budget curves for each metric
    for metric in ['expected_loss_gbp', 'rmse', 'mae']:
        plot_budget_curves(
            all_results, metric,
            output_dir / "curves" / f"m2_budget_{metric}.png",
            show_ci=True
        )

    # Performance profile
    perf_data = {}
    for method in method_names:
        perf_data[method] = {
            f"k{k}": all_results[method][k]['aggregated']['expected_loss_gbp']['mean']
            for k in cfg.selection.budgets
        }

    plot_performance_profile(
        perf_data, 'expected_loss_gbp',
        output_dir / "curves" / "m2_performance_profile.png"
    )

    # Critical difference diagram
    plot_critical_difference(
        perf_data, 'expected_loss_gbp',
        output_dir / "curves" / "m2_critical_difference.png"
    )

    # Check acceptance criterion
    greedy_mi_loss = all_results['Greedy-MI'][cfg.selection.budgets[-1]]['aggregated']['expected_loss_gbp']['mean']
    random_loss = all_results['Random'][cfg.selection.budgets[-1]]['aggregated']['expected_loss_gbp']['mean']

    improvement = (random_loss - greedy_mi_loss) / random_loss
    passes_m2 = improvement >= cfg.acceptance.m2_min_improvement_vs_random

    print("\n" + "-"*60)
    print(f"M2 RESULTS:")
    print(f"  Greedy-MI loss: Â£{greedy_mi_loss:.0f}")
    print(f"  Random loss: Â£{random_loss:.0f}")
    print(f"  Improvement: {improvement*100:.1f}%")
    print(f"  Threshold: {cfg.acceptance.m2_min_improvement_vs_random*100:.1f}%")
    print(f"  Status: {'âœ“ PASS' if passes_m2 else 'âœ— FAIL'}")
    print("-"*60)

    m2_summary = {
        'passes': passes_m2,
        'improvement': improvement,
        'greedy_mi_loss': greedy_mi_loss,
        'random_loss': random_loss
    }


    with open(output_dir / "m2_summary.json", 'w', encoding='utf-8') as f:
        json.dump(m2_summary, f, indent=2, cls=NumpyEncoder)

    try:
        generate_all_expert_plots(all_results, cfg, geom, sensors, output_dir)
    except Exception as e:
        print(f"âš ï¸  Visualization generation failed: {e}")
        import traceback
        traceback.print_exc()

    return m2_summary, all_results


def generate_all_expert_plots(all_results, cfg, geom, sensors, output_dir):
    """
    ç”Ÿæˆä¸“å®¶å»ºè®®çš„æ‰€æœ‰å›¾åƒï¼ˆæ”¹è¿›çš„é”™è¯¯å¤„ç†ç‰ˆæœ¬ï¼‰
    """
    from visualization import (
        setup_style,
        plot_budget_curves,
        plot_marginal_efficiency,
        plot_type_composition,
        plot_mi_voi_correlation,
        plot_calibration_diagnostics,
        plot_spatial_diagnostics_enhanced,
        plot_performance_profile,
        plot_critical_difference,
        plot_sensor_placement_map
    )
    import traceback

    print("\n" + "=" * 60)
    print("GENERATING EXPERT-RECOMMENDED VISUALIZATIONS")
    print("=" * 60)

    setup_style()

    # åˆ›å»ºè¾“å‡ºå­ç›®å½•
    curves_dir = output_dir / "curves"
    maps_dir = output_dir / "maps"
    calibration_dir = output_dir / "calibration"
    comparison_dir = output_dir / "comparison"

    for d in [curves_dir, maps_dir, calibration_dir, comparison_dir]:
        d.mkdir(exist_ok=True)

    # ========== F1: é¢„ç®—-æŸå¤±å‰æ²¿ ==========
    print("\nğŸ“ˆ F1: Budget-Loss Frontier...")
    try:
        results_by_method = {}
        for method_name in all_results.keys():
            results_by_method[method_name] = {}
            for budget in all_results[method_name].keys():
                results_by_method[method_name][budget] = all_results[method_name][budget]

        plot_budget_curves(
            results_by_method,
            metric='expected_loss_gbp',
            output_path=curves_dir / "f1_budget_loss_frontier.png",
            show_ci=True
        )
        print("  âœ… F1 saved")
    except Exception as e:
        print(f"  âŒ F1 failed: {e}")
        traceback.print_exc()

    # ========== F2 + F3 + F10: éœ€è¦ selection_result ==========
    print("\nğŸ’° F2, ğŸ¨ F3, ğŸ—ºï¸ F10: Processing selection results...")

    for method_name in ['Greedy-MI', 'Greedy-A']:
        if method_name not in all_results:
            print(f"  â­ï¸ Skipping {method_name} (not in results)")
            continue

        budgets = sorted(all_results[method_name].keys())

        for budget in budgets:
            result_data = all_results[method_name][budget]

            # ğŸ”¥ è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®ç»“æ„
            print(f"\n  Debug {method_name} k={budget}:")
            print(f"    Keys in result_data: {list(result_data.keys())}")

            if 'fold_results' not in result_data:
                print(f"    âŒ No fold_results")
                continue

            fold_results = result_data['fold_results']
            print(f"    Number of folds: {len(fold_results)}")

            if len(fold_results) == 0:
                print(f"    âŒ Empty fold_results")
                continue

            # æ£€æŸ¥ç¬¬ä¸€ä¸ªfold
            fold_0 = fold_results[0]
            print(f"    Keys in fold_0: {list(fold_0.keys())}")

            if 'selection_result' not in fold_0:
                print(f"    âŒ No selection_result in fold")
                continue

            selection_result = fold_0['selection_result']
            print(f"    âœ… Found selection_result with {len(selection_result.selected_ids)} sensors")

            # ========== F2: å•ä½æˆæœ¬æ•ˆç‡ ==========
            try:
                print(f"\n  ğŸ’° Generating F2 for {method_name} k={budget}...")
                plot_marginal_efficiency(
                    selection_result,
                    sensors,
                    output_path=curves_dir / f"f2_efficiency_{method_name.lower().replace('-', '_')}_k{budget}.png"
                )
                print(f"    âœ… F2 saved")
            except Exception as e:
                print(f"    âŒ F2 failed: {e}")
                traceback.print_exc()

            # ========== F3: ç±»å‹å †å å›¾ ==========
            try:
                print(f"\n  ğŸ¨ Generating F3 for {method_name} k={budget}...")
                plot_type_composition(
                    selection_result,
                    sensors,
                    output_path=curves_dir / f"f3_type_composition_{method_name.lower().replace('-', '_')}_k{budget}.png"
                )
                print(f"    âœ… F3 saved")
            except Exception as e:
                print(f"    âŒ F3 failed: {e}")
                traceback.print_exc()

            # ========== F10: é€‰å€åœ°å›¾ ==========
            try:
                print(f"\n  ğŸ—ºï¸ Generating F10 for {method_name} k={budget}...")
                plot_sensor_placement_map(
                    geom.coords,
                    selection_result.selected_ids,
                    sensors,
                    output_path=maps_dir / f"f10_placement_{method_name.lower().replace('-', '_')}_k{budget}.png"
                )
                print(f"    âœ… F10 saved")
            except Exception as e:
                print(f"    âŒ F10 failed: {e}")
                traceback.print_exc()

    # ========== F4: MI vs VoI ç›¸å…³æ€§ ==========
    print("\nğŸ”— F4: MI vs VoI Correlation...")
    print("  â­ï¸ F4 skipped (requires EVI computation)")

    # ========== F5: æ ¡å‡†è¯Šæ–­ ==========
    print("\nğŸ“Š F5: Calibration Diagnostics...")
    try:
        for method_name in ['Greedy-MI']:
            if method_name not in all_results:
                continue

            all_fold_results = []
            for budget in all_results[method_name].keys():
                result_data = all_results[method_name][budget]
                if 'fold_results' in result_data:
                    all_fold_results.extend(result_data['fold_results'])

            if all_fold_results:
                print(f"  Processing {len(all_fold_results)} folds for {method_name}...")
                plot_calibration_diagnostics(
                    all_fold_results,
                    output_path=calibration_dir / f"f5_calibration_{method_name.lower().replace('-', '_')}.png"
                )
                print(f"  âœ… F5 saved")
            else:
                print(f"  âŒ No fold results for {method_name}")
    except Exception as e:
        print(f"  âŒ F5 failed: {e}")
        traceback.print_exc()

    # ========== F6: ç©ºé—´è¯Šæ–­ ==========
    print("\nğŸ—ºï¸ F6: Spatial Diagnostics...")
    try:
        for method_name in ['Greedy-MI', 'Random']:
            if method_name not in all_results:
                continue

            budgets = list(all_results[method_name].keys())
            mid_budget = budgets[len(budgets) // 2]
            result_data = all_results[method_name][mid_budget]

            if 'fold_results' in result_data and len(result_data['fold_results']) > 0:
                fold_0 = result_data['fold_results'][0]

                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦çš„æ•°æ®
                if all(k in fold_0 for k in ['mu_post', 'x_true', 'test_idx']):
                    print(f"  Generating F6 for {method_name}...")
                    plot_spatial_diagnostics_enhanced(
                        fold_0['mu_post'],
                        fold_0['x_true'],
                        geom.coords,
                        fold_0['test_idx'],
                        method_name,
                        output_path=maps_dir / f"f6_spatial_{method_name.lower().replace('-', '_')}.png"
                    )
                    print(f"  âœ… F6 for {method_name} saved")
                else:
                    print(f"  â­ï¸ F6 for {method_name} skipped (missing data)")
    except Exception as e:
        print(f"  âŒ F6 failed: {e}")
        traceback.print_exc()

    # ========== F7: æ€§èƒ½å‰–é¢ + CDå›¾ ==========
    print("\nğŸ“‰ F7: Performance Profile & Critical Difference...")
    try:
        perf_data = {}
        for method_name in all_results.keys():
            perf_data[method_name] = {}
            for budget in all_results[method_name].keys():
                instance_key = f"k={budget}"
                agg = all_results[method_name][budget]['aggregated']
                perf_data[method_name][instance_key] = agg['expected_loss_gbp']['mean']

        plot_performance_profile(
            perf_data,
            metric='expected_loss_gbp',
            output_path=comparison_dir / "f7a_performance_profile.png",
            tau_max=3.0
        )

        plot_critical_difference(
            perf_data,
            metric='expected_loss_gbp',
            output_path=comparison_dir / "f7b_critical_difference.png",
            alpha=0.05
        )
        print("  âœ… F7 saved")
    except Exception as e:
        print(f"  âŒ F7 failed: {e}")
        traceback.print_exc()

    # ========== F8 & F9 ==========
    print("\nğŸ”¬ F8: Ablation Study...")
    print("  â­ï¸ F8 skipped (requires separate experiments)")

    print("\nâ±ï¸ F9: Complexity Analysis...")
    print("  â­ï¸ F9 skipped (requires timing instrumentation)")

    print("\n" + "=" * 60)
    print("VISUALIZATION COMPLETE")
    print("=" * 60)
    print(f"\nğŸ“‚ All plots saved to: {output_dir}")


def run_full_experiment(cfg):
    """Run complete experimental pipeline."""
    print("\n" + "="*70)
    print(" RDT-VoI SIMULATION: FULL EXPERIMENTAL PIPELINE")
    print("="*70)
    print(f"\nExperiment: {cfg.experiment.name}")
    print(f"Random seed: {cfg.experiment.seed}")
    print(f"Output directory: {cfg.experiment.output_dir}")

    # Create output directory
    output_dir = create_output_dir(cfg)
    print(f"\nOutput directory: {output_dir}")

    # Setup visualization
    setup_style(cfg.plots.style)

    # Run milestones
    try:
        # M1: Small-scale validation
        m1_summary = run_milestone_m1(cfg, output_dir)

        # M2: Full comparison
        m2_summary, all_results = run_milestone_m2(cfg, output_dir)

        # Final summary
        print("\n" + "="*70)
        print(" EXPERIMENT COMPLETE")
        print("="*70)
        print(f"\nAll results saved to: {output_dir}")
        print("\nMilestone Status:")
        print(f"  M1 (Monotonicity): {'âœ“ PASS' if m1_summary['monotonic'] else 'âœ— FAIL'}")
        print(f"  M1 (Diminishing):  {'âœ“ PASS' if m1_summary['diminishing'] else 'âœ— FAIL'}")
        print(f"  M2 (Improvement):  {'âœ“ PASS' if m2_summary['passes'] else 'âœ— FAIL'}")

        # Write final summary
        final_summary = {
            'experiment_name': cfg.experiment.name,
            'timestamp': datetime.now().isoformat(),
            'milestones': {
                'M1': m1_summary,
                'M2': m2_summary
            },
            'output_directory': str(output_dir)
        }

        with open(output_dir / "final_summary.json", 'w', encoding='utf-8') as f:
            json.dump(final_summary, f, indent=2, cls=NumpyEncoder)

        print(f"\nâœ“ Summary saved: {output_dir / 'final_summary.json'}")

    except Exception as e:
        print(f"\nâœ— ERROR: {e}")
        import traceback
        traceback.print_exc()

        # Save error log
        with open(output_dir / "error.log", 'w', encoding='utf-8') as f:
            traceback.print_exc(file=f)

        raise


def main():
    """Main evaluation pipeline."""

    print("=" * 70)
    print("  RDT-VoI EVALUATION FRAMEWORK")
    print("=" * 70)

    # ========================================================================
    # 1. SETUP
    # ========================================================================
    print("\n[1] Loading configuration...")
    cfg = load_config()
    rng = cfg.get_rng()

    # Create output directory
    output_dir = Path(cfg.experiment.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"    Output directory: {output_dir}")

    # Save config copy
    cfg.save_to(output_dir)

    # ========================================================================
    # 2. BUILD SPATIAL DOMAIN
    # ========================================================================
    print("\n[2] Building spatial domain...")
    geom = build_grid2d_geometry(cfg.geometry.nx, cfg.geometry.ny, cfg.geometry.h)

    # ========================================================================
    # 3. CONSTRUCT PRIOR
    # ========================================================================
    print("\n[3] Constructing GMRF prior with DDI control...")

    # ğŸ”¥ å¦‚æœé…ç½®äº†ç›®æ ‡ DDIï¼Œä½¿ç”¨å¢å¼ºç‰ˆæ„å»ºå‡½æ•°
    if hasattr(cfg.decision, 'target_ddi') and cfg.decision.target_ddi > 0:
        # å…ˆè·å–é˜ˆå€¼
        from spatial_field import build_prior_with_ddi

        # ä¸´æ—¶æ„å»ºä¸€æ¬¡å…ˆéªŒä»¥è·å–åˆå§‹ mu
        Q_temp, mu_temp = build_prior(geom, cfg.prior)
        tau = cfg.decision.get_threshold(mu_temp)

        print(f"    Target DDI: {cfg.decision.target_ddi:.2%}")
        print(f"    Decision threshold: Ï„ = {tau:.3f}")

        # ä½¿ç”¨ DDI æ§åˆ¶é‡æ–°æ„å»º
        Q_pr, mu_pr = build_prior_with_ddi(
            geom, cfg.prior,
            tau=tau,
            target_ddi=cfg.decision.target_ddi
        )
    else:
        Q_pr, mu_pr = build_prior(geom, cfg.prior)

    # ç»˜åˆ¶ DDI çƒ­åŠ›å›¾
    if cfg.plots.ddi_overlay.get('enable', True):
        from spatial_field import plot_ddi_heatmap, compute_ddi
        from inference import SparseFactor, compute_posterior_variance_diagonal

        # ä¼°è®¡å…ˆéªŒæ ‡å‡†å·®
        factor = SparseFactor(Q_pr)
        sample_idx = rng.choice(geom.n, size=min(100, geom.n), replace=False)
        sample_vars = compute_posterior_variance_diagonal(factor, sample_idx)
        sigma_est = np.sqrt(np.mean(sample_vars)) * np.ones(geom.n)

        tau = cfg.decision.get_threshold(mu_pr)

        plot_ddi_heatmap(
            geom, mu_pr, sigma_est, tau,
            output_path=output_dir / 'ddi_heatmap_prior.png'
        )
    print(f"    Precision sparsity: {Q_pr.nnz / geom.n ** 2 * 100:.2f}%")
    print(f"    Correlation length: {cfg.prior.correlation_length:.1f} m")

    # ========================================================================
    # 4. SAMPLE TRUE STATE
    # ========================================================================
    print("\n[4] Sampling true deterioration state...")
    x_true = sample_gmrf(Q_pr, mu_pr, rng)
    print(f"    State range: [{x_true.min():.2f}, {x_true.max():.2f}]")
    print(f"    Mean: {x_true.mean():.2f}, Std: {x_true.std():.2f}")

    # Save true state
    np.save(output_dir / 'x_true.npy', x_true)

    # ========================================================================
    # 5. GENERATE SENSOR POOL
    # ========================================================================
    print("\n[5] Generating sensor pool...")

    if cfg.sensors.use_heterogeneous:
        # ğŸ”¥ ä½¿ç”¨å¼‚è´¨åŒ–ä¼ æ„Ÿå™¨ç”Ÿæˆ
        from sensors import generate_heterogeneous_sensor_pool, create_cost_zones_example

        if cfg.sensors.cost_zones:
            cost_zones = cfg.sensors.cost_zones
        else:
            cost_zones = create_cost_zones_example(geom)

        sensors = generate_heterogeneous_sensor_pool(
            geom, cfg.sensors,
            cost_zones=cost_zones,
            rng=rng
        )
    else:
        sensors = generate_sensor_pool(geom, cfg.sensors, rng)

    # ========================================================================
    # 6. PREPARE TEST SET (for EVI and diagnostics)
    # ========================================================================
    print("\n[6] Preparing global test set for EVI...")
    n_test = min(200, geom.n)
    test_idx_global = rng.choice(geom.n, size=n_test, replace=False)
    print(f"    Test set size: {n_test}")

    # ========================================================================
    # 7. RUN METHOD EVALUATIONS
    # ========================================================================
    print("\n[7] Running method evaluations...")

    methods = get_available_methods(cfg)
    print(f"    Methods to evaluate: {', '.join(methods)}")

    all_results = {}

    for method_name in methods:
        t_method_start = time.time()

        try:
            results = run_method_evaluation(
                method_name=method_name,
                cfg=cfg,  # ğŸ”¥ ç¡®ä¿å‚æ•°åä¸å‡½æ•°å®šä¹‰åŒ¹é…
                geom=geom,
                Q_pr=Q_pr,
                mu_pr=mu_pr,
                x_true=x_true,
                sensors=sensors,
                test_idx_global=test_idx_global
            )
            all_results[method_name] = results

            t_method_elapsed = time.time() - t_method_start
            print(f"\n  âœ“ {method_name} completed in {t_method_elapsed:.1f}s")

        except Exception as e:
            print(f"\n  âœ— {method_name} FAILED: {str(e)}")
            warnings.warn(f"Method {method_name} failed: {str(e)}")
            import traceback
            traceback.print_exc()
            continue

    # ========================================================================
    # 8. SAVE RESULTS
    # ========================================================================
    print("\n[8] Saving results...")

    # Save raw results
    with open(output_dir / 'results_raw.pkl', 'wb') as f:
        pickle.dump(all_results, f)
    print(f"    Saved: results_raw.pkl")

    # ğŸ”¥ Convert to DataFrame with error handling
    print("    Converting results to DataFrame...")
    try:
        df_results = aggregate_results_for_visualization(all_results)

        if df_results.empty:
            print("    âš ï¸  Warning: No results to aggregate (all methods may have failed)")
        else:
            df_results.to_csv(output_dir / 'results_aggregated.csv', index=False)
            print(f"    Saved: results_aggregated.csv ({len(df_results)} rows)")
    except Exception as e:
        print(f"    âœ— Failed to create aggregated DataFrame: {e}")
        df_results = pd.DataFrame()  # Empty DataFrame

    # ========================================================================
    # 9. VISUALIZATION
    # ========================================================================
    print("\n[9] Generating visualizations...")

    plots_dir = output_dir / 'plots'
    plots_dir.mkdir(exist_ok=True)

    # ğŸ”¥ åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰å°è¯•å¯è§†åŒ–
    if df_results.empty:
        print("    âš ï¸  Skipping visualization: no data available")
    else:
        try:
            # F1: Budget-performance curves
            print("    [F1] Budget-performance curves...")
            try:
                fig = plot_budget_curves(
                    df_results,
                    output_dir=plots_dir,
                    config=cfg
                )
                if fig:
                    plt.close(fig)
            except Exception as e:
                print(f"      Skipped: {e}")

            # F2: Marginal efficiency
            print("    [F2] Marginal efficiency...")
            try:
                fig = plot_marginal_efficiency(
                    all_results,
                    output_dir=plots_dir,
                    config=cfg
                )
                if fig:
                    plt.close(fig)
            except Exception as e:
                print(f"      Skipped: {e}")

            # F7a: Performance profile
            print("    [F7a] Performance profile...")
            try:
                fig = plot_performance_profile(
                    df_results,
                    metric='expected_loss_gbp',
                    output_dir=plots_dir,
                    config=cfg
                )
                if fig:
                    plt.close(fig)
            except Exception as e:
                print(f"      Skipped: {e}")

            # F7b: Critical difference diagram
            print("    [F7b] Critical difference diagram...")
            try:
                fig = plot_critical_difference(
                    df_results,
                    metric='expected_loss_gbp',
                    output_dir=plots_dir,
                    config=cfg
                )
                if fig:
                    plt.close(fig)
            except Exception as e:
                print(f"      Skipped: {e}")

            # F4: MI-EVI correlation (if both methods ran)
            if 'greedy_mi' in all_results and 'greedy_evi' in all_results:
                print("    [F4] MI-EVI correlation...")
                try:
                    fig = plot_mi_evi_correlation(
                        all_results['greedy_mi'],
                        all_results['greedy_evi'],
                        output_dir=plots_dir,
                        config=cfg
                    )
                    if fig:
                        plt.close(fig)
                except Exception as e:
                    print(f"      Skipped: {e}")

            # F5: Calibration diagnostics
            print("    [F5] Calibration diagnostics...")
            try:
                fig = plot_calibration_diagnostics(
                    all_results,
                    output_dir=plots_dir,
                    config=cfg
                )
                if fig:
                    plt.close(fig)
            except Exception as e:
                print(f"      Skipped: {e}")

            print(f"    âœ“ Visualization complete")

        except Exception as e:
            print(f"    âœ— Visualization phase failed: {str(e)}")
            import traceback
            traceback.print_exc()

    generate_expert_plots(
        all_results=all_results,
        sensors=sensors,
        geom=geom,
        Q_pr=Q_pr,
        output_dir=output_dir,
        config=cfg
    )
    # ========================================================================
    # 10. SUMMARY REPORT
    # ========================================================================
    print("\n" + "=" * 70)
    print("  EVALUATION COMPLETE")
    print("=" * 70)

    print("\nSummary:")
    print(f"  Methods attempted: {len(methods)}")
    print(f"  Methods completed: {len(all_results)}")
    print(f"  Budgets: {cfg.selection.budgets}")
    print(f"  CV folds: {cfg.cv.k_folds}")
    print(f"  Total domain size: {geom.n}")
    print(f"  Candidate pool: {len(sensors)}")

    print(f"\nResults saved to: {output_dir}")
    print(f"  - results_raw.pkl")
    if not df_results.empty:
        print(f"  - results_aggregated.csv")
        print(f"  - plots/ directory")

    # Print best method per metric (if data available)
    if not df_results.empty and 30 in cfg.selection.budgets:
        print("\nBest methods (at k=30):")
        for metric in ['rmse', 'expected_loss_gbp', 'coverage_90']:
            try:
                df_k30 = df_results[
                    (df_results['budget'] == 30) &
                    (df_results['metric'] == metric)
                    ]
                if not df_k30.empty:
                    if metric == 'coverage_90':
                        # Closer to 0.90 is better
                        df_k30 = df_k30.copy()
                        df_k30['score'] = -np.abs(df_k30['mean'] - 0.90)
                        best_row = df_k30.loc[df_k30['score'].idxmax()]
                    else:
                        # Lower is better
                        best_row = df_k30.loc[df_k30['mean'].idxmin()]
                    print(f"  {metric:20s}: {best_row['method']:15s} "
                          f"({best_row['mean']:.3f} Â± {best_row['std']:.3f})")
            except Exception:
                pass

    print("\n" + "=" * 70)


if __name__ == "__main__":
    main()